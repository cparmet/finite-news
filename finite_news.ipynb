{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8021432a-564d-4e72-9085-500fbfb4b010",
   "metadata": {},
   "source": [
    "# Welcome to Finite News!\n",
    "This notebook creates and emails issues of Finite News. To facilitate simple scheduling of the notebook with Sagemaker, all the code lives inside the notebook. Papermill jobs cannot import local Python scripts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8b8248-8001-4308-abed-ef4c4d644986",
   "metadata": {},
   "source": [
    "# Parameters for the notebook\n",
    "The constants below are used for developing and debugging. All other parameters for the newspaper are configured in the files on S3.  \n",
    "  \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p><b>To deploy from dev to prod:</b></p>\n",
    "    <ol>\n",
    "        <li>Set <code>DEV_MODE</code> & <code>DISABLE_GPT</code> to <code>False</code>.</li>\n",
    "        <li>Create a new <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/create-notebook-auto-run-studio.html\" target=\"_blank\">scheduled notebook job</a> on the Data Science 2.0 image with Python 3.8.</li>\n",
    "        <li>Delete the old notebook job if any.</li>\n",
    "        <li>Shut down the Sagemaker instance if it was used during development.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69927ffe-ffbd-4472-9df9-e26cac0940a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEV_MODE = False # True will not send email and not cache newly fetched headlines for dedup later\n",
    "DISABLE_GPT = False # True will not call GPT API, so we don't incur costs while debugging\n",
    "LOGGING_LEVEL = \"warning\" # What level of logging should go in admin's issue/local log file?\n",
    "                       # Use \"warning\" by default. \n",
    "                       # Use \"info\" to get more detailed FN messages for debugging\n",
    "                       # Use \"debug\" to get lower-level messages from dependencies\n",
    "                       # Reminder: When re-running notebook, to change logging level for example, restart kernel. Logger can only initialize once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd609d66-2217-4f08-bab1-d01d770e2c0c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e3404b-21fc-4f9a-a41a-5ca2a731daca",
   "metadata": {},
   "source": [
    "The next cell installs specific versions of packages that I've found play nice together on SageMaker, both local runs and scheduled jobs using `Python 3.8` on an `ml.m5.large` instance and the `Data Science 2.0` image.  \n",
    "  \n",
    "‚ÑπÔ∏è If you're running FiniteNews in a different environment and you get errors here:\n",
    "1. Try finding different versions that play nicely in your environment. Start by `pip` installing offending packages without a pinned version number and see which versions `pip` found.\n",
    "2. If you're still getting errors on `sentence-transformers` (or its dependencies like `pytorch` or `GLIBCXX`), try a different environment. For example it may help to use the SageMaker image for `Pytorch 1.12 Python 3.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8175d1b-1bf3-4edf-993e-1f38d5826c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet beautifulsoup4==4.12.2 boto3==1.33.9 botocore==1.33.9 env_canada==0.6.1 emoji==2.12.1 \\\n",
    "feedparser==6.0.11 ipywidgets==7.6.5 jupyterlab_widgets==1.0.0 openai==0.27.7 \\\n",
    "pandas==1.3.4 s3fs==2024.2.0 seaborn==0.11.2 sendgrid==6.10.0 sentence-transformers==2.3.1 \\\n",
    "widgetsnbextension==3.5.1 yfinance==0.2.33 matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0445b477-a51e-494f-8ed6-733a95c3637f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import base64\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from bs4 import BeautifulSoup\n",
    "import calendar\n",
    "from copy import deepcopy\n",
    "from datetime import date, datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import emoji\n",
    "from env_canada import ECWeather\n",
    "import feedparser\n",
    "from io import BytesIO, StringIO\n",
    "from itertools import combinations\n",
    "import json\n",
    "import logging\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.text import Text\n",
    "%config InlineBackend.figure_format = 'svg' # Makes plots higher quality\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() # Allows us to use async libraries like env_canada easily within the notebook using asyncio.run()\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from random import choice\n",
    "import requests\n",
    "import s3fs\n",
    "fs = s3fs.S3FileSystem()\n",
    "import seaborn as sns\n",
    "from sendgrid import Attachment, SendGridAPIClient\n",
    "from sendgrid.helpers.mail import Mail\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "import traceback\n",
    "import yaml\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daccdb53-a46c-40e0-a8d3-105856b027e9",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b025080-1156-49ba-99e1-bd4d00a47afd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üì¶ Load\n",
    "Import data and initialize variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29968326-8855-4d51-9543-116c56f5eec6",
   "metadata": {},
   "source": [
    "### General assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294f7c7-888b-4ece-be99-0ed6064929f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_logging(logging_level, dev_mode):\n",
    "    \"\"\"Initialize logging to either \n",
    "        * (default) in-memory object (for optional delivery in admin's issue of Finite News) or\n",
    "        * (if dev_mode=True) a local log file\n",
    "    \n",
    "    NOTE\n",
    "    Reminder: This function doesn't reset an active log. Must restart the kernel in SageMaker.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    logging_level (str): The granularity of logging messages, 'warning', 'info' or 'debug'. If dev_mode=True, forced to 'debug'\n",
    "    dev_mode (bool): If False, we're in prod mode and logs will go to log_stream. If True, will send logs to local file\n",
    "    \n",
    "    RETURNS\n",
    "    log_stream (StringIO object): If dev_mode=False, returns in-memory file-like object that collects results from logging during the Finite News run\n",
    "    \"\"\"\n",
    "    \n",
    "    if logging_level=='warning':\n",
    "        level = logging.WARNING\n",
    "    elif logging_level=='info':\n",
    "        level = logging.INFO\n",
    "    elif logging_level=='debug':\n",
    "        level = logging.DEBUG\n",
    "\n",
    "    if dev_mode:\n",
    "        # Local file \n",
    "        logging.basicConfig(\n",
    "            filename='app.log',\n",
    "            level=level,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        return None\n",
    "    else:\n",
    "        # Create in-memory file-like object\n",
    "        log_stream = StringIO() \n",
    "        logging.basicConfig(stream=log_stream, level=level)\n",
    "        return log_stream\n",
    "\n",
    "\n",
    "def get_fn_secret(secret_key, secret_name=\"fn_secrets\", region_name=\"us-east-1\"):\n",
    "    \"\"\"Retrieve a secret from AWS Secrets Manager.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    secret_key (string): the specific secret to retrieve, such as BUCKET_PATH or OPENAI_API_KEY\n",
    "    secret_name (string): the group where the Finite News secrets are stored in AWS Secrets Manager\n",
    "    region_name (string): the region where your AWS Secrets Manager secret_name lives. See the sample code provided by Secrets Manager after you create the secret\n",
    "\n",
    "    RETURNS\n",
    "    secret_value (string): the secret!\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e: # Stop the presses, we can't get our secret.\n",
    "        raise e\n",
    "\n",
    "    # Decrypt secret using the associated KMS key.\n",
    "    try:\n",
    "        return json.loads(get_secret_value_response[\"SecretString\"])[secret_key]\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"Secret key {str(e)} not found. Is it stored in AWS Secrets Manager? Have you given permissions for your SageMaker user to access the secret?\") # No sense in logging the exception since we won't be sending any emails (where we store logs)\n",
    "        \n",
    "\n",
    "def load_s3(bucket_path, file_path, required=True):\n",
    "    \"\"\"Loads a file from S3 into Python variable\n",
    "    \n",
    "    ARGUMENTS\n",
    "    bucket_path (str): The location of the S3 bucket where required files are stored.\n",
    "    file_path (str): The name or path of the file\n",
    "    required (bool): Should we error out if we can't load it?\n",
    "    \n",
    "    RETURNS\n",
    "    variable\n",
    "    \"\"\"\n",
    "    file_format = file_path.split(\".\")[-1]\n",
    "    try:\n",
    "        with fs.open(bucket_path + file_path, \"r\") as f:\n",
    "            if file_format==\"yml\":\n",
    "                variable = yaml.load(f, Loader=yaml.Loader)\n",
    "            elif file_format==\"htm\" or file_format==\"html\":\n",
    "                variable = f.read()\n",
    "            elif file_format==\"txt\":\n",
    "                variable = f.readlines()\n",
    "            else:\n",
    "                logging.warning(f\"Unsupported file type in load_s3: {file_path}\")\n",
    "                return None\n",
    "        logging.info(f\"Read {file_path} from S3\")\n",
    "        return variable\n",
    "                \n",
    "    except Exception as e:\n",
    "        error_message = f\"Couldn't load {file_path} from S3. {str(type(e))}, {str(e)}\"\n",
    "        if required: \n",
    "            logging.critical(error_message)\n",
    "            raise(e)\n",
    "        logging.warning(error_message)\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_publication_config(\n",
    "    publication_config_file_name=\"publication_config.yml\",\n",
    "    dev_mode=False,\n",
    "    disable_gpt=False\n",
    "):\n",
    "    \"\"\"Import general settings and assets from files on S3, used for all subscribers\n",
    "    \n",
    "    ARGUMENTS\n",
    "    publication_config_file_name (str): file name for the general publication parameters YML file in the S3 bucket identified by BUCKET_PATH\n",
    "    dev_mode (bool): If True we're in development or debug mode, so don't send emails or modify headline_logs.\n",
    "    disable_gpt (bool): If True, don't call the GPT API and incur costs, for example during dev or debug cycles.\n",
    "    \n",
    "    RETURNS\n",
    "    publication_config (dict): General settings for all subscribers \n",
    "    \"\"\"\n",
    "    \n",
    "    bucket_path = get_fn_secret(\"BUCKET_PATH\")\n",
    "\n",
    "    # Load publication settings\n",
    "    publication_config = load_s3(bucket_path, publication_config_file_name)\n",
    "    \n",
    "    # Populate config dictionary, loading more assets as needed\n",
    "    if publication_config[\"editorial\"].get(\"enable_thoughts_of_the_day\", False):\n",
    "        thoughts_of_the_day = load_s3(bucket_path, \"thoughts_of_the_day.yml\", required=False)\n",
    "        if thoughts_of_the_day: \n",
    "            thoughts_of_the_day = thoughts_of_the_day[\"quotes\"]\n",
    "    else:\n",
    "        thoughts_of_the_day = []\n",
    "    \n",
    "    return {\n",
    "        \"bucket_path\": bucket_path,\n",
    "        \"email_delivery\": not dev_mode, # If dev_mode is True, don't send emails\n",
    "        \"sender\": publication_config[\"sender\"],\n",
    "        \"layout\": {\n",
    "            \"template_html\": load_s3(bucket_path, \"template.htm\", \"r\"),\n",
    "            \"logo_url\": publication_config[\"layout\"][\"logo_url\"],\n",
    "        },\n",
    "        \"editorial\": {\n",
    "            \"one_headline_keywords\": publication_config[\"editorial\"].get(\"one_headline_keywords\", []),\n",
    "            \"substance_rules\": load_s3(bucket_path, \"substance_rules.yml\"),\n",
    "            \"cache_issue_content\": False if dev_mode else True,\n",
    "            \"gpt\": publication_config[\"editorial\"].get(\"gpt\", None) if not disable_gpt else None,\n",
    "            \"smart_deduper\": publication_config[\"editorial\"].get(\"smart_deduper\", None),\n",
    "            \"enable_thoughts_of_the_day\": publication_config[\"editorial\"].get(\"enable_thoughts_of_the_day\", False)\n",
    "        },\n",
    "        \"forecast\" : publication_config.get(\"forecast\", {}),\n",
    "        \"news_sources\": publication_config[\"news_sources\"],\n",
    "        \"events_sources\": publication_config.get(\"events_sources\", []),\n",
    "        \"alerts_sources\": publication_config.get(\"alerts_sources\", []),\n",
    "        \"image_sources\": publication_config.get(\"image_sources\", []),\n",
    "        \"thoughts_of_the_day\": thoughts_of_the_day\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7d0249-9674-4ef6-9729-2217d84d0685",
   "metadata": {},
   "source": [
    "### Subscriber assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e30960-a4e5-495d-97f4-687011c55e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subscriber_list(bucket_path, folder_name=\"finite_files\"):\n",
    "    \"\"\"Find the subscribers (the names of their config files) on the Finite News bucket.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    bucket_path (str): The location of the S3 bucket where required files are stored.\n",
    "    folder_name (str): The part of the path that contains the folder on the bucket, if present. Used to remove from .\n",
    "    \n",
    "    NOTE: \n",
    "    1. Assumes the folder is at the root of the bucket. If it's nested, use relative path up to root.\n",
    "    2. Assumes all files in the folder that begin with \"config_\" are a subscriber config file.\n",
    "    \n",
    "    RETURNS\n",
    "    subscriber_config_file_names (list): yml file names in finite bucket\n",
    "    \"\"\"\n",
    "    \n",
    "    fn_bucket = (\n",
    "        boto3\n",
    "        .resource(\"s3\")\n",
    "        .Bucket(\n",
    "            bucket_path\n",
    "            .split(\"//\")\n",
    "            [1]\n",
    "            .split(\"/\")\n",
    "            [0]\n",
    "        )\n",
    "    )\n",
    "    # Iterate through files on the bucket and select those that begin with config_\n",
    "    return [\n",
    "        f.key.replace(f\"{folder_name}/\", \"\")\n",
    "        for f in fn_bucket.objects.filter(Prefix=f\"{folder_name}/\")\n",
    "        if f.key.startswith(f\"{folder_name}/config_\")\n",
    "    ]\n",
    "\n",
    "\n",
    "def filter_sources(sources, selections, criterion=\"name\"):\n",
    "    \"\"\"Applies subscriber's selections to list of sources\n",
    "\n",
    "    ARGUMENTS\n",
    "    sources (list of dict): Descriptions of sources from publication_config\n",
    "    selections (list of str): Names/Categories of sources that subscriber wants\n",
    "    criterion (str): \"name\" or \"category\" for how to filter\n",
    "    \n",
    "    RETURNS\n",
    "    sources_filtered (list of dict): Subset of sources that match subscriber's selections\n",
    "    \"\"\"\n",
    "    if not selections:\n",
    "        filtered_sources = []\n",
    "    else:\n",
    "        # Get the source details from publication config that were in susbcriber's selections\n",
    "        # while keeping the order of subscriber's selections\n",
    "        filtered_sources = sorted(\n",
    "            [source for source in sources if source[criterion] in selections],\n",
    "            key=lambda x: selections.index(x[criterion])\n",
    "        )\n",
    "    logging.info(f\"Filtered out sources not in {selections}: {[source['name'] for source in sources if source[criterion] not in selections]}\")\n",
    "    return filtered_sources\n",
    "\n",
    "\n",
    "def day_name_to_number(day_name):\n",
    "    \"\"\"Helper function to convert a named day like \"Friday\" to an ISO standard number like 4.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    day_name (str): Fully spelled out day of week. Case insensitive\n",
    "    \n",
    "    RETURNS\n",
    "    day_number (int): Number from 0-6, where 0 = Monday\n",
    "    \"\"\"\n",
    "    calendar.Calendar(firstweekday=0)\n",
    "    return (\n",
    "        {name: i for i, name in enumerate(calendar.day_name)}\n",
    "        .get(day_name.capitalize(), None)\n",
    "        + 1  # To align with isocalendar()\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_frequency_config(frequency_config):\n",
    "    \"\"\"Determine if today is the day to deliver a scheduled section of the paper.\n",
    "    \n",
    "    NOTE\n",
    "    * Reminder: When adding new frequencies, update get_stocks_plot()\n",
    "    * Assumes the paper is delivered once per day. So \"daily\" config always returns True.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    frequency_config (dict): Parameters for a cycle\n",
    "    \n",
    "    RETURNS\n",
    "    match (bool): Is today on the schedule?\n",
    "    \"\"\"\n",
    "    \n",
    "    if not frequency_config:\n",
    "        logging.warning(\"Missing frequency config, assumed to be False\")\n",
    "        return False\n",
    "    \n",
    "    frequency = frequency_config.get(\"frequency\", None) # The cadence label\n",
    "        \n",
    "    if frequency == \"monthly\":\n",
    "        dom = frequency_config.get(\"day_of_month\", 1) # Which day of the month does subscriber want?\n",
    "        dom_today = date.today().day # What's the day of the month today?\n",
    "        match = dom == dom_today\n",
    "        logging.info(f\"parse_frequency_config, result: {match}. Today: {dom_today}. Requested: {dom}\")\n",
    "        return match\n",
    "    \n",
    "    if frequency == \"weekly\":\n",
    "        dow_number = day_name_to_number(frequency_config.get(\"day_of_week\", \"Monday\"))\n",
    "        _, today_dow_number = date.today().isocalendar()[1:] # Get today's \"week of year\" and \"day of week\" as integers using ISO standard\n",
    "        match = today_dow_number==dow_number # Is today the requested day of the week?\n",
    "        logging.info(f\"parse_frequency_config, result: {match}. Today dow number: {today_dow_number}. Requested: {frequency_config.get('day_of_week')}, dow_number: {dow_number}\")\n",
    "        return match\n",
    "\n",
    "    if frequency == \"every_other_week\":\n",
    "        dow_number = day_name_to_number(frequency_config.get(\"day_of_week\", \"Monday\"))\n",
    "        eow_odd = frequency_config.get(\"eow_odd\", False) # Should every other week fall on odd week numbers or even?\n",
    "        week_number, today_dow_number = date.today().isocalendar()[1:] # Get today's \"week of year\" and \"day of week\" as integers using ISO standard\n",
    "        week_number_match = (\n",
    "                (eow_odd and week_number % 2 == 1)\n",
    "                or (not eow_odd and week_number % 2 == 0)\n",
    "        )\n",
    "        match = (\n",
    "            today_dow_number==dow_number # Today is the requested day of the week\n",
    "            and week_number_match # This is the requested week\n",
    "        )\n",
    "        logging.info(f\"parse_frequency_config, result: {match}. Today week_number, dow_number: {week_number, today_dow_number}. Requested: dow_number: {dow_number}, eow_odd: {eow_odd}\")\n",
    "        return match\n",
    "\n",
    "    if frequency == \"daily\":\n",
    "        logging.info(f\"parse_frequency_config, result: True, because 'daily' is always True\")\n",
    "        return True\n",
    "\n",
    "    if frequency == \"weekdays\":\n",
    "        match = date.today().isoweekday()<6\n",
    "        logging.info(f\"parse_frequency_config, result: {match}, requested: weekeday (iso number <6), today: {date.today().isoweekday()}.\")\n",
    "        return match\n",
    "\n",
    "    else:\n",
    "        logging.warning(f\"Unexpected value for frequency: {frequency}. Not parsed.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_events_config(publication_events_sources, subscriber_sources):\n",
    "    \"\"\"Import the parameters for an events calendar source, if subscriber requests. \n",
    "    \n",
    "    Includes deciding if today meets the subscriber's frequency for including events in their issue.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    publication_events_sources (list of dict): The source config for event-type sources in the publication, if present\n",
    "    subscriber_sources (list of str): The subscriber's source configuration, which may or may not include preferences for event sources\n",
    "    \n",
    "    RETURNS\n",
    "    event_sources (list of dict): Source configuration for events that \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        subscriber_events_sources = subscriber_sources.get(\"events\", {}).get(\"sources\", [])\n",
    "        frequency_match = parse_frequency_config(\n",
    "            subscriber_sources.get(\"events\", {}).get(\"frequency\", {\"frequency\":\"daily\"})\n",
    "        )\n",
    "        if frequency_match and len(publication_events_sources)>0 and len(subscriber_events_sources)>0:\n",
    "            return filter_sources(publication_events_sources, subscriber_events_sources)\n",
    "        else:\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Unhandled exception in load_events_config: {str(type(e))}, {str(e)}. publication_events_sources: {publication_events_sources}. subscriber_sources: {subscriber_sources}\")\n",
    "        return []\n",
    "        \n",
    "        \n",
    "def load_stocks_config(subscriber_sources):\n",
    "    \"\"\"Import the parameters for subscriber's stock section, if any.\n",
    "\n",
    "    ARGUMENTS\n",
    "    subscriber_sources (list of str): The subscriber's source configuration, which may or may not include preferences for stock data\n",
    "    \n",
    "    RETURNS\n",
    "    stocks (list of lists): Lists of tickers for each plot [ [TICKER1, TICKER2], [TICKER3, TICKER4] ], or empty list for none\n",
    "    frequency (str): How often we are delivering this section. Used to determine how much history to put in plot\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        stocks_config = subscriber_sources.get(\"stocks\", None)\n",
    "        if not stocks_config:\n",
    "            return [], None\n",
    "        frequency_match = parse_frequency_config(stocks_config)\n",
    "        frequency = stocks_config.get(\"frequency\", None)\n",
    "        ticker_sets = stocks_config.get(\"tickers\", [])\n",
    "        if len(ticker_sets)==0 or not frequency_match:\n",
    "            return [], None\n",
    "        return [[ticker.strip() for ticker in ticker_set.split(\",\")] for ticker_set in ticker_sets], frequency\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Unhandled exception in load_stocks_confg: {str(type(e))}, {str(e)}. subscriber_sources: {subscriber_sources}\")\n",
    "        return [], None\n",
    "    \n",
    "\n",
    "def load_subscriber_config(subscriber_config_file_name, publication_config):\n",
    "    \"\"\"Import subscriber-specific parameters and combine with general publication settings\n",
    "    \n",
    "    ARGUMENTS\n",
    "    subscriber_config_file_name (str): name of the subscriber's config YML file in the S3 bucket\n",
    "    publication_config (dict): loaded general publication parameters\n",
    "    \n",
    "    RETURNS\n",
    "    issue (dict): Settings for an issue, combining subscriber and general publication parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transfer general settings from publication config\n",
    "    issue = deepcopy(publication_config) # Copy dict with nested dicts\n",
    "    \n",
    "    # Load subscriber's specific settings\n",
    "    subscriber_config = load_s3(issue[\"bucket_path\"], subscriber_config_file_name)\n",
    "    \n",
    "    # Check are we delivering this issue today?\n",
    "    if not parse_frequency_config(\n",
    "        subscriber_config.get(\"issue_frequency\", {\"frequency\":\"daily\"})\n",
    "    ):\n",
    "        logging.info(f\"{subscriber_config['email']}: No issue today, not in issue_frequency.\")\n",
    "        return None\n",
    "    \n",
    "    issue[\"admin\"] = subscriber_config.get(\"admin\", False)\n",
    "    issue[\"sender\"][\"subject\"] = subscriber_config[\"editorial\"].get(\"subject\", \"Finite News\")\n",
    "    issue[\"subscriber_email\"] = subscriber_config[\"email\"]\n",
    "\n",
    "    issue[\"editorial\"][\"add_car_talk_credit\"] = subscriber_config[\"editorial\"].get(\"add_car_talk_credit\", False)\n",
    "    issue[\"editorial\"][\"cache_path\"] = subscriber_config.get(\"editorial\", {}).get(\"cache_path\", \"\")\n",
    "    if issue[\"editorial\"][\"cache_path\"] == \"\":\n",
    "        logging.warning(\"No cache_path. Not logging new content or removing content already presented in last year.\")\n",
    "    else:\n",
    "        # If cache file doesn't exist, create empty file\n",
    "        if not fs.exists(issue[\"bucket_path\"] + issue[\"editorial\"][\"cache_path\"]):\n",
    "            with fs.open(issue[\"bucket_path\"] + issue[\"editorial\"][\"cache_path\"], \"wb\") as f:\n",
    "                f.write(b\"\")\n",
    "    issue[\"requests_timeout\"] = subscriber_config.get(\"editorial\",{}).get(\"requests_timeout\", 30)\n",
    "    \n",
    "    issue[\"news_sources\"] = filter_sources(\n",
    "        issue[\"news_sources\"],\n",
    "        subscriber_config.get(\"sources\", {}).get(\"news_categories\", []),\n",
    "        \"category\"\n",
    "    )\n",
    "    issue[\"events_sources\"] = load_events_config(publication_config[\"events_sources\"], subscriber_config[\"sources\"])\n",
    "    issue[\"alerts_sources\"] = filter_sources(\n",
    "        issue[\"alerts_sources\"],\n",
    "        subscriber_config.get(\"sources\", {}).get(\"alerts_sources\", []),\n",
    "        \"name\"\n",
    "    )\n",
    "    issue[\"alerts_sources\"] += [\n",
    "        {\n",
    "            \"name\": \"MBTA API: Alerts\",\n",
    "            \"type\": \"mbta_alerts\",\n",
    "            \"route\": mbta_source.get(\"route\", None),\n",
    "            \"stations\": mbta_source.get(\"stations\", []),\n",
    "            \"direction_id\": mbta_source.get(\"direction_id\", None),\n",
    "        }\n",
    "        for mbta_source in subscriber_config.get(\"sources\",{}).get(\"mbta\",[])\n",
    "        if parse_frequency_config(mbta_source)\n",
    "    ]\n",
    "    issue[\"image_sources\"] = filter_sources(\n",
    "        issue[\"image_sources\"],\n",
    "        subscriber_config.get(\"sources\", {}).get(\"image_categories\", []),\n",
    "        \"category\"\n",
    "    )          \n",
    "    issue[\"stocks\"], issue[\"stocks_frequency\"] = load_stocks_config(subscriber_config[\"sources\"])\n",
    "    issue[\"sports\"] = subscriber_config.get(\"sports\", {})\n",
    "    issue[\"forecast\"] = subscriber_config.get(\"forecast\", {})\n",
    "    if issue[\"forecast\"]:\n",
    "        issue[\"forecast\"][\"api_snooze_bar\"] = publication_config[\"forecast\"].get(\"api_snooze_bar\", None)\n",
    "        \n",
    "    issue[\"slogans\"] = subscriber_config[\"slogans\"]\n",
    "    if publication_config[\"editorial\"][\"enable_thoughts_of_the_day\"]:\n",
    "        issue[\"thoughts_of_the_day\"] = subscriber_config.get(\"thoughts_of_the_day\", [])\n",
    "        if issue[\"thoughts_of_the_day\"] and subscriber_config[\"editorial\"].get(\"add_shared_thoughts\", False):\n",
    "            issue[\"thoughts_of_the_day\"] += publication_config[\"thoughts_of_the_day\"] \n",
    "    else: \n",
    "        issue[\"thoughts_of_the_day\"] = []\n",
    "    return issue\n",
    "\n",
    "\n",
    "def load_subscriber_configs(dev_mode, disable_gpt):\n",
    "    \"\"\"Create the config file needed to generate each issue, combining publication and subscriber settings.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    dev_mode (bool): If True we're in development or debug mode, so don't send emails or modify headline_logs.\n",
    "    disable_gpt (bool): If True, don't call the GPT API and incur costs, for example during dev or debug cycles.\n",
    "\n",
    "    RETURNS\n",
    "    subscriber_configs (list): issue_config for each subscriber we need to generate an issue for\n",
    "    \"\"\" \n",
    "    \n",
    "    publication_config = load_publication_config(dev_mode=dev_mode, disable_gpt=disable_gpt)    \n",
    "    subscriber_list = get_subscriber_list(publication_config[\"bucket_path\"])\n",
    "    subscriber_configs = [\n",
    "        load_subscriber_config(subscriber_config_file_name, publication_config)\n",
    "        for subscriber_config_file_name in subscriber_list\n",
    "    ]\n",
    "    subscriber_configs = [c for c in subscriber_configs if c is not None] # Drop Nones, which occur if today is not in the issue_frequency for that subscriber\n",
    "    \n",
    "    # Sort subscribers so the \"admins\" go last. \n",
    "    # Allows the admin email issue(s) to include logging warnings from the non-admin issues.\n",
    "    subscriber_configs = sorted(subscriber_configs , key=lambda x: x[\"admin\"]) \n",
    "    return subscriber_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf9d023-4383-4387-a634-46f3fa7efd0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üïµüèª‚Äç‚ôÄÔ∏è Report\n",
    "Research the content for an issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ded448-d384-4851-97bf-4a9c616153c4",
   "metadata": {},
   "source": [
    "### General reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b00531-2df9-4ae8-b6b8-bfde78d8921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup(l):\n",
    "    \"\"\"De-duplicate a list while preserving the order of elements, unlike list(set()).\n",
    "    \n",
    "    ARGUMENTS\n",
    "    l (list): A list of items\n",
    "    \n",
    "    RETURNS\n",
    "    l_dedup (list): The list in its original order, but without dups\n",
    "    \n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    return [x for x in l if not (x in seen or seen.add(x))]\n",
    "\n",
    "\n",
    "def heal_inner_n(s):\n",
    "    \"\"\"Replace one or more inner \\n with a colon. \n",
    "    \n",
    "    NOTES\n",
    "    Assumes \\n have been removed from ends\n",
    "    \n",
    "    ARGUMENTS\n",
    "    s (str): A string with or without one or more \\n in the middle\n",
    "    \n",
    "    RETURNS\n",
    "    string with any \\n in the middle replaced with a \": \"\n",
    "    \"\"\"\n",
    "    \n",
    "    if \"\\n\" in s:\n",
    "        return s.split(\"\\n\")[0] + \": \" + s.split(\"\\n\")[-1]\n",
    "    return s\n",
    "\n",
    "\n",
    "def create_calendar_sitemap_url(base_url, path_format, substract_one_day):\n",
    "    \"\"\"Generate a URL for sites that organize content chronologically in site-map.\n",
    "    \n",
    "    Useful for sites where there's no good way to ensure a page with headlines ordered by recency,\n",
    "    where normal scraping would lead to old headlines reemerging\"\n",
    "        \n",
    "    ARGUMENTS\n",
    "    base_url (str): The core part of the URL that we'll add onto, e.g. \"http://www.website.com/sitemap/\"\n",
    "    path_format (str): The format for the path pointing to the date we want to hit. e.g. \"full_year/month_lower/day\"\n",
    "        Supported elements (in any order): full_year (2024), month_lower (august), month_title_case (August), day (5)\n",
    "    subtract_one_day (bool): Whether to traverse to yesterday's date instead of today\n",
    "    \n",
    "    RETURNS\n",
    "    string with fully specified url to the date desired, e.g. \"http://www.website.com/sitemap/2024/august/5\"\n",
    "    \"\"\"\n",
    "    target_date = date.today() - timedelta(days=1) if substract_one_day else date.today()\n",
    "    return (\n",
    "        base_url\n",
    "        + path_format\n",
    "        \n",
    "        # Replace supported \n",
    "        .replace(\"full_year\", str(target_date.year))\n",
    "        .replace(\"month_lower\", target_date.strftime(\"%B\").lower())\n",
    "        .replace(\"month_title_case\", target_date.strftime(\"%B\").title()) # Initial-capitalized\n",
    "        .replace(\"day\", str(target_date.day))\n",
    "    )\n",
    "\n",
    "\n",
    "def scrape_source(source, requests_timeout, retry=True):\n",
    "    \"\"\"Fetch and parse the HTML tags from a web location.\n",
    "        \n",
    "    ARGUMENTS\n",
    "    source (dict): Description of the website to scrape\n",
    "    requests_timeout (int): Number of seconds to wait before giving up on an HTTP request\n",
    "    retry (bool): Whether to run the request again if no items were scraped\n",
    "    \n",
    "    RETURNS\n",
    "    items (list of str): Text retrieved\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        if \"calendar_sitemap_format\" in source:\n",
    "            url = create_calendar_sitemap_url(source[\"url\"], source.get(\"calendar_sitemap_format\"), source.get(\"calendar_sitemap_subtract_one_day\", False))\n",
    "        else:\n",
    "            url = source[\"url\"]\n",
    "\n",
    "        if source.get(\"specify_request_headers\", False):\n",
    "            headers = {\n",
    "                'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'\n",
    "            }\n",
    "        else:\n",
    "            headers = None\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=requests_timeout)\n",
    "        except requests.exceptions.SSLError as e:\n",
    "            logging.warning(f\"SSL error on {source['name']}, {url}. {str(type(e))}, {str(e)}\")\n",
    "            return []\n",
    "        except requests.exceptions.Timeout as e:\n",
    "            logging.warning(f\"Request timed out after {requests_timeout} seconds: {url}. More details: {source['name']}, {str(type(e))}, {str(e)}\")\n",
    "            return []        \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Requests error on {source['name']}, {url}. {str(type(e))}, {str(e)}\")\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(response.text, features=source.get(\"parser\", \"html.parser\"))\n",
    "\n",
    "        if \"select_query\" in source: # Scrape the content using a BeautifulSoup query\n",
    "            items = soup.select(source[\"select_query\"])\n",
    "        elif \"tag_class\" in source: # Scrape the content by finding tags with a specific class\n",
    "            items = soup.find_all(source[\"tag\"], {\"class\":source[\"tag_class\"]})\n",
    "        elif \"multitag_group\" in source: # Scrape the content by finding repeating groups of tags and combine the text of each set of tags.\n",
    "            groups = soup.find_all(source[\"multitag_group\"])\n",
    "            separator = source.get(\"multitag_separator\", \" \")\n",
    "            for i, tag in enumerate(source[\"multitag_tags\"]): # Iteratively append text from multiple consecutive tags into each string\n",
    "                if i==0:\n",
    "                    items = [f\"{group.find(tag).text}\" for group in groups]\n",
    "                else:\n",
    "                    try:\n",
    "                        items = [f\"{item_text}{separator}{group.find(tag).text}\" for item_text, group in zip(items, groups)]\n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"multitag error on {source['name']} while appending tag {tag}. Maybe tag not present for all items? {str(type(e))}, {str(e)}\")\n",
    "        else:\n",
    "            items = soup.find_all(source[\"tag\"])\n",
    "\n",
    "        if \"tag_next\" in source: # Scrape a specified tag that appears _after_ \"tag\"\n",
    "            items = items[0].findNext(source[\"tag_next\"])\n",
    "\n",
    "        if \"detail_page_root\" in source: # Scrape a child page\n",
    "            # We have to go depper!\n",
    "            detail_link = items[0].attrs[\"href\"]\n",
    "            header = items[0].get_text().strip()\n",
    "            # Request a detail page\n",
    "            response = requests.get(source[\"detail_page_root\"] + detail_link, headers=headers, timeout=requests_timeout)\n",
    "            soup = BeautifulSoup(response.text, features=source.get(\"parser\", \"html.parser\"))\n",
    "\n",
    "            # Get the detail image\n",
    "            img_element = soup.find_all(\"img\")[source[\"detail_img_number\"]-1]\n",
    "            alt = img_element.attrs['alt']\n",
    "            src = img_element.attrs['src']\n",
    "            # TODO: Make the following reuse code above\n",
    "            if \"detail_text_tag_class\" in source:\n",
    "                text = soup.find_all(source[\"detail_text_tag\"], {\"class\":source[\"detail_text_tag_class\"]})[0].get_text().strip()\n",
    "            elif \"detail_text_tag\" in source:\n",
    "                text = soup.find_all(source[\"detail_text_tag\"])[0].get_text().strip()\n",
    "            else:\n",
    "                text=\"\"\n",
    "\n",
    "            if source.get(\"add_http_img\", False):\n",
    "                src = f\"http:{src}\"\n",
    "            items = [f\"\"\"<h4>{header}</h4><img alt=\"{alt}\" src=\"{src}\"><p>{text}</p>\"\"\"]\n",
    "        elif \"split_char\" in source:\n",
    "            items = [item for item in items.get_text().split(source[\"split_char\"]) if item]    \n",
    "        elif \"multitag_group\" not in source: # multitag scraping at this stage already has text for each item. The other approaches need us to extract the text.\n",
    "            items = [item.get_text() for item in items]\n",
    "\n",
    "        # If at first you don't succeed...try just one more time\n",
    "        # Some sources are finnicky and work better with two swings\n",
    "        # But don't retry if this is a source we expect 0 results often\n",
    "        if not items and retry and not source.get(\"exclude_from_0_results_warning\", False):\n",
    "            logging.info(f\"No items scraped. Waiting 3 seconds and retrying.... {source['name']}\")\n",
    "            sleep(3)\n",
    "            scrape_source(source, requests_timeout, retry=False)\n",
    "\n",
    "        # Apply certain text cleaning that depends on source config\n",
    "        # TODO: Move these to editing; keep items associated with their source config longer\n",
    "        # Also because then user can apply these configs to API sources, not just scrapes\n",
    "\n",
    "        # Check if certain phrases are present/absent\n",
    "        if \"must_contain\" in source:\n",
    "            # When it's a list, it's an OR\n",
    "            if type(source[\"must_contain\"])==list:\n",
    "                items = [\n",
    "                    h for h in items \n",
    "                    if sum([must_contain.lower() in h.lower() for must_contain in source[\"must_contain\"]])>0\n",
    "                        ]\n",
    "            else:\n",
    "                items = [h for h in items if source[\"must_contain\"].lower() in h.lower()] \n",
    "        if \"cant_contain\" in source:\n",
    "            if type(source[\"cant_contain\"])==list:\n",
    "                cant_contains = source[\"cant_contain\"]\n",
    "            else:\n",
    "                cant_contains = [source[\"cant_contain\"]]\n",
    "            for cant_contain in cant_contains:\n",
    "                items = [h for h in items if cant_contain.lower() not in h.lower()]\n",
    "\n",
    "        # Clean text\n",
    "        if \"remove_text\" in source:\n",
    "            items = [h.replace(source[\"remove_text\"],\"\") for h in items]\n",
    "\n",
    "        # Remove \\n and \\t from ends of strings. Needed before heal_inner_n\n",
    "        precleaning = True\n",
    "        while precleaning:\n",
    "            original_len = sum([len(h) for h in items])\n",
    "            items = [h.strip(\"\\r\").strip(\"\\n\").strip(\"\\t\") for h in items]\n",
    "            precleaning = (original_len != sum([len(h) for h in items]))\n",
    "\n",
    "        # Clean strings with a \"\\n\" in the middle\n",
    "        if \"heal_inner_n\" in source:\n",
    "            items = [heal_inner_n(item) for item in items]\n",
    "\n",
    "        # Ensure each string is long enough.\n",
    "        if \"min_words\" in source:\n",
    "            items = [item for item in items if len(item.strip().split(\" \"))>=source[\"min_words\"]] # simple way to count words\n",
    "\n",
    "        return dedup(items)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Source failed on {source['name']}. {str(type(e))}, {str(e)}. Source: {source}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def research_source(source, requests_timeout):\n",
    "    \"\"\"Get a source's content, whether from API or scraping, and format into desired structure.\n",
    "\n",
    "    NOTE\n",
    "    See also clean_headline() for post-processing that's done on the level of an individual headline.\n",
    "\n",
    "    ARGUMENTS\n",
    "    source (dict): Description of the API to call or website to scrape\n",
    "    requests_timeout (int): Number of seconds to wait before giving up on an HTTP request\n",
    "\n",
    "    RETURNS\n",
    "    items (list of str): Content fround from source\n",
    "    or\n",
    "    html (str): Formatted block of html\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get specialized content\n",
    "        if source[\"type\"] == \"events_calendar\":\n",
    "            return get_calendar_events(source, requests_timeout)\n",
    "        if source[\"type\"]==\"reminder\":\n",
    "            if parse_frequency_config(source.get(\"frequency\", None)):\n",
    "                return [source.get(\"reminder_message\", None)]\n",
    "            else:\n",
    "                return []\n",
    "        if source[\"type\"]==\"mbta_alerts\":\n",
    "            if not source[\"route\"] or not source[\"stations\"] or not source[\"direction_id\"]:\n",
    "                logging.warning(f\"mbta_alert not checked. Expected route, stations, and direction_id. Found: {source}\")\n",
    "                return []\n",
    "            return get_mbta_alerts(source[\"route\"], source[\"stations\"], source[\"direction_id\"], requests_timeout)\n",
    "\n",
    "        # Get general content\n",
    "        if source[\"method\"]==\"api\":\n",
    "            response = requests.get(source[\"url\"] + get_fn_secret(source[\"api_key_name\"]), timeout=requests_timeout)\n",
    "            items = [item[source[\"headline_field\"]] for item in response.json()[\"results\"]]\n",
    "        elif source[\"method\"]==\"scrape\":\n",
    "            items = scrape_source(source, requests_timeout)\n",
    "        elif source[\"method\"]==\"rss_images\":\n",
    "            if \"get_img_tag_under_this_key\" in source:\n",
    "                html_blocks = [\n",
    "                    entry[source[\"get_img_tag_under_this_key\"]]\n",
    "                    for entry in feedparser.parse(source[\"url\"]).entries \n",
    "                ]  \n",
    "                # Extract the first <img> tag in each html_block\n",
    "                items = [f'''\n",
    "                        <h4>{source.get(\"header\",\"\")}</h4>\n",
    "                        {BeautifulSoup(html_block, \"html.parser\").find(\"img\")}'''\n",
    "                        for html_block in html_blocks\n",
    "                ] \n",
    "                # Drop items that have no <img> component, only <h4>\n",
    "                items = [item for item in items if \"<img\" in item]\n",
    "            else:\n",
    "                urls = [\n",
    "                    entry[\"media_content\"][0].get(\"url\", None)\n",
    "                    for entry in feedparser.parse(source[\"url\"]).entries \n",
    "                    if \"media_content\" in entry\n",
    "                ]   \n",
    "                items = [f'''\n",
    "                        <h4>{source.get(\"header\",\"\")}</h4>\n",
    "                        <img src=\"{url}\">'''\n",
    "                        for url in urls\n",
    "                ]\n",
    "        elif source[\"method\"]==\"atom\":\n",
    "            newest_entry = feedparser.parse(source[\"url\"]).entries[0]\n",
    "            if \"header_path\" in source:\n",
    "                header = newest_entry\n",
    "                for node in source[\"header_path\"]:\n",
    "                    header = header[node]\n",
    "                header = f\"<h4>{source.get('header_preface', '')}{header}</h4>\"\n",
    "            else:\n",
    "                header = \"\"\n",
    "            if \"image_path\" in source:\n",
    "                img = newest_entry\n",
    "                for node in source[\"image_path\"]:\n",
    "                    img = img[node]\n",
    "            else:\n",
    "                img = \"\"\n",
    "            if \"body_path\" in source:\n",
    "                body = newest_entry\n",
    "                for node in source[\"body_path\"]:\n",
    "                    body = body[node]\n",
    "                body = f\"<p>{body}</p>\"\n",
    "            else:\n",
    "                body = \"\"\n",
    "            items = [f\"\"\"{header}{img}{body}\"\"\"]\n",
    "            \n",
    "        # Lightly postprocess results\n",
    "        if items:\n",
    "            items = [item.replace(\"\\n\",\"\").strip() for item in items if item]\n",
    "            max_items = source.get(\"max_items\", source.get(\"max_headlines\", None)) # The attribute can have either name \n",
    "            items = items[0:max_items]\n",
    "        # Log count\n",
    "        if len(items)==0 and not source.get(\"exclude_from_0_results_warning\", False): # Escalate to admin if no results were returned, and that was unexpected. Source's scraper/API may be broken.\n",
    "            logging.warning(f\"{source['name']}: retrieved 0 items\") \n",
    "        else:\n",
    "            logging.info(f\"{source['name']}: retrieved {len(items)} items\")\n",
    "\n",
    "        # Add prefaces and return\n",
    "        if source[\"type\"] in [\"headlines\", \"image_url\"]:\n",
    "            return [f\"{source.get('preface','')}{item}\" for item in items] # Add preface, if requested\n",
    "        elif source[\"type\"] == \"alert_new\":\n",
    "            return [\n",
    "                f\"\"\"{source.get('alert_preface', '')} <a href=\"{source['url']}\" target=\"_blank\">{item}</a>\"\"\" \n",
    "                for item in items\n",
    "            ] # Wrap the alert in a URL. Add preface, if requested (add separately from regular 'preface', to isolate item)\n",
    "        else:\n",
    "            logging.warning(f\"Unknown type of source {source['type']}: {str(source)}\") \n",
    "            return []\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error getting content from source {source['name']}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def get_attributions(general_sources, sports_tracked, weather_source, stocks_used, car_talk_used):\n",
    "    \"\"\"Compile the names of all sources used in the issue, to give credit.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    general_sources (list of dict): A list of sources we tried to get news, alerts, etc from\n",
    "    sports_tracked (dict): Sources we tried to get sports from\n",
    "    weather_source (str): The forecast \"source\" attribute from subsciber's config, or None\n",
    "    stocks_used (bool): True if we included stock data\n",
    "    car_talk_used (bool): True if subscriber includes Car Talk credits\n",
    "    \n",
    "    RETURNS\n",
    "    attributions (list of str): The names of the sources\n",
    "    \"\"\"\n",
    "    attributions = list(set([source[\"name\"] for source in general_sources])) # De-dups and sorts\n",
    "    if \"nba_teams\" in sports_tracked: attributions += [\"NBA API\"]\n",
    "    if \"nhl_teams\" in sports_tracked: attributions += [\"NHL API\"]\n",
    "    if weather_source==\"nws\": attributions += [\"National Weather Service API\"]\n",
    "    if weather_source==\"env_canada\": attributions += [\"Environment Canada Weather API\"]\n",
    "    if stocks_used: attributions += [\"Yahoo Finance API\"]\n",
    "    if car_talk_used: attributions += [\"Car Talk credits\"]\n",
    "    return sorted(attributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e781662e-6979-481e-9e93-5ab95af62402",
   "metadata": {},
   "source": [
    "### Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264c36b-9033-41aa-b275-29edd6e7b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_todays_nba_game(team_name, requests_timeout):\n",
    "    \"\"\"Call the NBA API to find out if a team is playing today.\n",
    "    \n",
    "    NOTE\n",
    "    This updated version accounts for the limitation of using the NBA API's current day's scoreboard: \n",
    "    the scoreboard isn't always updated until a certain hour in the morning, after FN may be run.\n",
    "    The updated approach here looks at the whole year's schedule, including post-season. Adapted from : https://github.com/swar/nba_api/issues/296\n",
    "\n",
    "    TODO: Clean and simpify. No need to use Pandas.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "    team_name (str): NBA team such as \"Celtics\" or \"Lakers\"\n",
    "    requests_timeout (int): Number of seconds to wait before giving up on an HTTP request\n",
    "\n",
    "    RETURNS\n",
    "    message (str or None): A headline-style update if the team is playing tonight.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        url = 'https://cdn.nba.com/static/json/staticData/scheduleLeagueV2.json'\n",
    "        r = requests.get(url, timeout=requests_timeout)\n",
    "        schedule = r.json()\n",
    "        schedule = schedule['leagueSchedule']['gameDates']\n",
    "        games = []\n",
    "        for gameday in schedule:\n",
    "            for game in gameday['games']:\n",
    "                game_details = [\n",
    "                        game['gameDateTimeUTC'],\n",
    "                        game['homeTeam']['teamName'],\n",
    "                        game['homeTeam']['teamCity'],\n",
    "                        game['awayTeam']['teamName'],\n",
    "                        game['awayTeam']['teamCity'],\n",
    "                       ]\n",
    "                game_details = pd.DataFrame(\n",
    "                    [game_details],\n",
    "                    columns =[\n",
    "                        \"gameDateTimeUTC\",\n",
    "                        \"homeTeam\",\n",
    "                        \"homeCity\",\n",
    "                        \"awayTeam\",\n",
    "                        \"awayCity\",\n",
    "                    ]\n",
    "                )\n",
    "                games.append(game_details)\n",
    "        \n",
    "        if not games: # A day with no games in the league\n",
    "            return None\n",
    "\n",
    "        games = pd.concat([game for game in games])\n",
    "        \n",
    "\n",
    "        eastern = pytz.timezone('US/Eastern')\n",
    "        games['gameDateTimeUTC'] = pd.to_datetime(games['gameDateTimeUTC'], errors='coerce')\n",
    "        games = games.dropna(subset=['gameDateTimeUTC'])\n",
    "        games['gameDateTimeEastern'] = games['gameDateTimeUTC'].apply(lambda t: t.astimezone(eastern))\n",
    "        games['gameDate'] = games['gameDateTimeEastern'].apply(lambda d: d.date())\n",
    "\n",
    "        game = (\n",
    "            games.loc[\n",
    "                ((games['awayTeam'] == team_name) | (games['homeTeam'] == team_name))\n",
    "                & (games['gameDate'] == datetime.today().date())]\n",
    "        )\n",
    "        if game.shape[0]==1:\n",
    "            game = game.iloc[0]\n",
    "            tipoff = game[\"gameDateTimeEastern\"].strftime(\"%I:%M\").lstrip(\"0\").replace(\":00\",\"\")\n",
    "            if team_name in game[\"homeTeam\"]:\n",
    "                other_team = game[\"awayTeam\"]\n",
    "                return f\"üèÄ The {team_name} host the {other_team} at {tipoff}.\"\n",
    "            else:\n",
    "                other_city = game[\"homeCity\"]\n",
    "                return f\"üèÄ The {team_name} are in {other_city}. Tipoff at {tipoff}.\"\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"NBA game error for {team_name}: {str(type(e))}, {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_todays_nhl_game(team_place_name, requests_timeout):\n",
    "    \"\"\"Call the NHL API to find out if a team is playing today.\n",
    "    \n",
    "    TODO: Clean and simpify. No need to use Pandas.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "    team_place_name (str): the team's official named place, like Buffalo, Minnesota. For Montr√©al use the accented e. For New York, use team_place_name of Islanders or Rangers\n",
    "    requests_timeout (int): Number of seconds to wait before giving up on an HTTP request\n",
    "\n",
    "    RETURNS\n",
    "    message (str or None): A headline-style update if the team is playing tonight.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        url = \"https://api-web.nhle.com/v1/schedule/\" + date.today().strftime(\"%Y-%m-%d\")\n",
    "        r = requests.get(url, timeout=requests_timeout)\n",
    "        schedule = r.json()['gameWeek'][0]['games']\n",
    "        games = []\n",
    "        for game in schedule:\n",
    "            game_details = [\n",
    "                    game['startTimeUTC'],\n",
    "                    game['homeTeam']['placeName']['default'],\n",
    "                    game['awayTeam']['placeName']['default'],\n",
    "                   ]\n",
    "            game_details = pd.DataFrame(\n",
    "                [game_details],\n",
    "                columns =[\n",
    "                    \"gameDateTimeUTC\",\n",
    "                    \"home_place_name\",\n",
    "                    \"away_place_name\",\n",
    "                ]\n",
    "            )\n",
    "            games.append(game_details)\n",
    "        if not games: # A day with no games in the league\n",
    "            return None\n",
    "        games = pd.concat([game for game in games])\n",
    "        eastern = pytz.timezone('US/Eastern')\n",
    "        games['gameDateTimeUTC'] = pd.to_datetime(games['gameDateTimeUTC'], errors='coerce')\n",
    "        games = games.dropna(subset=['gameDateTimeUTC'])\n",
    "        games['gameDateTimeEastern'] = games['gameDateTimeUTC'].apply(lambda t: t.astimezone(eastern))\n",
    "        games['gameDate'] = games['gameDateTimeEastern'].apply(lambda d: d.date())\n",
    "\n",
    "        game = (\n",
    "            games.loc[\n",
    "                ((games['away_place_name'] == team_place_name) | (games['home_place_name'] == team_place_name))\n",
    "                & (games['gameDate'] == datetime.today().date())]\n",
    "        )\n",
    "        if game.shape[0]==1:\n",
    "            game = game.iloc[0]\n",
    "            tipoff = game[\"gameDateTimeEastern\"].strftime(\"%I:%M\").lstrip(\"0\").replace(\":00\",\"\")\n",
    "            if team_place_name in game[\"home_place_name\"]:\n",
    "                other_place_name = game[\"away_place_name\"]\n",
    "                if team_place_name in [\"Islanders\", \"Rangers\"]:\n",
    "                    team_place_name = f\"The {team_place_name} host\"\n",
    "                else:\n",
    "                    team_place_name += \" hosts\"\n",
    "                if other_place_name in [\"Islanders\", \"Rangers\"]:\n",
    "                    other_place_name = f\"the {other_place_name}\"\n",
    "                return f\"üèíü•Ö {team_place_name} {other_place_name}. They face off at {tipoff}.\"\n",
    "            else:\n",
    "                if team_place_name in [\"Islanders\", \"Rangers\"]:\n",
    "                    team_place_name = f\"The {team_place_name} are\"\n",
    "                else:\n",
    "                    team_place_name += \" skates\"\n",
    "                other_place_name = game[\"home_place_name\"]\n",
    "                if other_place_name in [\"Islanders\", \"Rangers\"]:\n",
    "                    other_place_name = f\"New York to face the {other_place_name}\"\n",
    "                return f\"üèíü•Ö {team_place_name} in {other_place_name}. The puck drops at {tipoff}.\"\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"NHL game error for {team_place_name}: {str(type(e))}, {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67859dd-aa1f-46c2-9c59-ce42904fba54",
   "metadata": {},
   "source": [
    "### Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6cbfa-13e8-4752-8c4d-6e96b959e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nws_forecast(nws_config):\n",
    "    \"\"\"Use National Weather Service API to get local forecast.\n",
    "        \n",
    "    NOTE: We use a fixed timeout for this API request, overriding publication_config's requests_timeout parameter.\n",
    "    The value here is based on experience with NWS possibly needing a number of API attempts to get a response\n",
    "    \n",
    "    ARGUMENTS\n",
    "    nws_config (dict): Parameters for calling the NWS API, including keys for:\n",
    "        - office (str): Which NWS office to get the forecast from (See NOTE above)\n",
    "        - grid_x (int), grid_y (int): Coordinates for the forecast (See NOTE above)\n",
    "        - location_name (str): Optional, Town or city name (no state/country etc)\n",
    "        - api_snooze_bar (int): How many seconds to wait before retrying NWS after an exception\n",
    "\n",
    "    RETURNS\n",
    "    forecast (dict or None): Attributes of the forecast retrieved, or None if there was a problem.\n",
    "    \"\"\"\n",
    "    \n",
    "    MAX_ATTEMPTS = 10 \n",
    "    try:\n",
    "        attempts=1\n",
    "        while attempts<MAX_ATTEMPTS:\n",
    "            url =f\"https://api.weather.gov/gridpoints/{nws_config['office']}/{nws_config['grid_x']},{nws_config['grid_y']}/forecast\"\n",
    "            r = requests.get(url, timeout=5)\n",
    "            if r.status_code==200:\n",
    "                break\n",
    "            else:\n",
    "                attempts+=1\n",
    "                logging.info(f\"Weather request {r.status_code}. Wait {nws_config['api_snooze_bar']} seconds and retry, take # {attempts} ...\")\n",
    "                sleep(nws_config[\"api_snooze_bar\"])\n",
    "        \n",
    "        # Get the next daytime forecast\n",
    "        # Traverse the list of forecast periods to find the first that isn't Overnight, ~Tuesday Night, Tonight, Evening  \n",
    "        daytime_forecasts = [\n",
    "            period for period in r.json()[\"properties\"][\"periods\"]\n",
    "            if \"night\" not in period['name'].lower() \n",
    "            and \"evening\" not in period['name'].lower()\n",
    "        ]\n",
    "        if not daytime_forecasts: # No daytime forecasts found\n",
    "            logging.warning(f\"No NWS forecast added because no non-night/overnight period available. Config: {news_config}. Response from NWS: {r.json()}.\")\n",
    "            print(\"error\")\n",
    "            return None\n",
    "        result = daytime_forecasts[0] # Get the daytime forecast that's coming first\n",
    "        \n",
    "        # Format forecast\n",
    "        forecast = {\n",
    "            \"short\": result.get(\"shortForecast\", None),\n",
    "            \"detailed\": result.get(\"detailedForecast\", None),\n",
    "            \"icon_url\": result.get(\"icon\", None)\n",
    "        }\n",
    "        forecast[\"short\"] = forecast[\"short\"].capitalize() # Change from Title Case to Sentence case \n",
    "        if \"location_name\" in nws_config:\n",
    "            forecast[\"short\"] += f\" in {nws_config['location_name']}\"\n",
    "        return forecast\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            logging.warning(f\"Forecast error after {MAX_ATTEMPTS} attempts: {str(type(e))}, {str(e)}, {r}\")\n",
    "        except UnboundLocalError:\n",
    "            logging.warning(f\"Forecast error after {MAX_ATTEMPTS} attempts: {str(type(e))}, {str(e)}. requests.get() did not return a response r.\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def get_ca_forecast(forecast_config):\n",
    "    \"\"\"Use Environment Canada API to get local forecast.\n",
    "        \n",
    "    ARGUMENTS\n",
    "    forecast_config (dict): Parameters for calling the env_canada API, including keys for:\n",
    "        - lat (float), lon (float): Coordinates for the forecast \n",
    "        - location_name (str): Optional, Town or city name (no province etc)\n",
    "\n",
    "    RETURNS\n",
    "    forecast (dict or None): Attributes of the forecast retrieved, or None if there was a problem.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        ec_en = ECWeather(coordinates=(forecast_config[\"lat\"], forecast_config[\"lon\"]))\n",
    "        asyncio.run(ec_en.update())\n",
    "        forecast_short, forecast_detailed = (\n",
    "            ec_en\n",
    "            .daily_forecasts\n",
    "            [0]\n",
    "            [\"text_summary\"]\n",
    "            .split(\".\", maxsplit=1)\n",
    "        )\n",
    "        # If the first forecast returned is a day forecast, add the second forecast -- tonight\n",
    "        # If the first forecast is a night forecast, don't add the second forecast. That's tomorrow's day forecast.\n",
    "        if \"night\" not in ec_en.daily_forecasts[0]['period'].lower():\n",
    "            forecast_detailed += f\"\\n\\nTonight: {ec_en.daily_forecasts[1]['text_summary']}\"            \n",
    "        forecast = {\n",
    "            \"short\": forecast_short,\n",
    "            \"detailed\": forecast_detailed,\n",
    "        }\n",
    "        if \"location_name\" in forecast_config:\n",
    "            forecast[\"short\"] += f\" in {forecast_config['location_name']}\"\n",
    "        return forecast\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"env_canada forecast error: {str(type(e))}, {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_gws_forecast(forecast_config):\n",
    "    \"\"\"Pull latest forecast from German Weather Service's Open Data. \n",
    "    \n",
    "    \n",
    "    ARGUMENTS\n",
    "    forecast_config (dict): Parameters for getting GWS data, including keys for:\n",
    "        - forecast_file (str): the name of the html file on \"https://opendata.dwd.de/weather/text_forecasts/html/\", the \"LATEST\" file for the desired region\n",
    "        - location_name (str): Optional, Town or city name (no province etc)\n",
    "        - api_timeout (int): Optional, Number of seconds to wait before giving up on a request\n",
    "\n",
    "        \n",
    "    RETURNS\n",
    "    forecast (dict or None): Attributes of the forecast retrieved, or None if there was a problem.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        forecast_config[\"url\"] = f\"https://opendata.dwd.de/weather/text_forecasts/html//{forecast_config['forecast_file']}\"\n",
    "        forecast_config[\"tag\"] = \"pre\"\n",
    "        \n",
    "        forecast = {\n",
    "            # Unlike NWS and env_canada, we don't have a brief forecast to use in the heading. :(\n",
    "            \"short\": \"Weather forecast\",\n",
    "            \n",
    "            # But we got a big honking forecast! Just needs some cleaning\n",
    "            \"detailed\": (\n",
    "                scrape_source(forecast_config, forecast_config.get(\"api_timeout\", 30))\n",
    "                [0]\n",
    "                .strip(\"\\r\\n\")\n",
    "                .replace(\"\\r\\n\\r\\n\", \"</p><p>\")\n",
    "                .replace(\"\\r\\n\", \" \")\n",
    "            )\n",
    "        }\n",
    "        if \"location_name\" in forecast_config:\n",
    "            forecast[\"short\"] += f\" for {forecast_config['location_name']}\"\n",
    "        return forecast\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"German Weather Service forecast error: {str(type(e))}, {str(e)}, {forecast_config}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def get_forecast(forecast_config):\n",
    "    \"\"\"Use selected API to get weather forecast\n",
    "    \n",
    "    ARGUMENT\n",
    "    forecast_config (dict): Parameters for calling the API, depending on \"source\"\n",
    "    \n",
    "    RETURNS\n",
    "    forecast (dict or None): Attributes of the forecast retrieved, or None if there was a problem.\n",
    "    \"\"\"\n",
    "    \n",
    "    if forecast_config[\"source\"] == \"env_canada\":\n",
    "        return get_ca_forecast(forecast_config)\n",
    "    elif forecast_config[\"source\"]== \"gws\":\n",
    "        return get_gws_forecast(forecast_config)\n",
    "    elif forecast_config[\"source\"] == \"nws\":\n",
    "        return get_nws_forecast(forecast_config)\n",
    "    else:\n",
    "        logging.warning(f\"Unexpected forecast source. No forecast added. {forecast_config}\") \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d820e-f88f-4e8b-84a4-ea99eb17be65",
   "metadata": {},
   "source": [
    "### Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097ea08-f029-4acf-86ce-208e61476ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_stock_history(ticker):\n",
    "    \"\"\"Retrieve the previous quarter of stock prices\n",
    "    \n",
    "    ARGUMENTS\n",
    "    ticker (str): The abbreviation of the stock\n",
    "    \n",
    "    RETURNS\n",
    "    stock_df (DataFrame): Previous month's stock prices\n",
    "    \"\"\"\n",
    "    stock = yf.Ticker(ticker)\n",
    "    \n",
    "    # Get stock name\n",
    "    # This is the series name that will be displayed in plot.\n",
    "    stock_info = stock.info\n",
    "    if \"shortName\" in stock_info:\n",
    "        stock_name = stock.info[\"shortName\"]\n",
    "    elif \"longName\" in stock_info:\n",
    "        stock_name = stock.info[\"longName\"]\n",
    "    else:\n",
    "        stock_name = ticker\n",
    "    if len(stock_name)<4: # If name is blank or unexpectedly short, use ticker\n",
    "        stock_name = ticker\n",
    "\n",
    "    # Get price series\n",
    "    # By default we get the last quarter, the max we may need\n",
    "    stock_df = (\n",
    "        stock\n",
    "        .history(period=\"3mo\")\n",
    "        .reset_index() #.reset_index()[[\"\n",
    "        .assign(\n",
    "            date = lambda df: df[\"Date\"].dt.strftime(\"%m-%d\"),\n",
    "        )\n",
    "        [[\"date\", \"Close\"]]\n",
    "    )\n",
    "    stock_df[stock_name] = stock_df[\"Close\"]\n",
    "    \n",
    "    return (\n",
    "        stock_df\n",
    "        .set_index([\"date\"])\n",
    "        .drop(columns=[\"Close\"])\n",
    "    )\n",
    "\n",
    "\n",
    "def research_stock_histories(tickers):\n",
    "    \"\"\"Get previous quarter prices for a list of stocks\n",
    "    \n",
    "    ARGUMENTS\n",
    "    tickers (list of str): The abbreviation (ticker) of each stock  \n",
    "\n",
    "    RETURNS\n",
    "    stocks_df (DataFrame): Previous month's prices, with each stock as a column and mon-day (str) as index\n",
    "    \"\"\"\n",
    "    \n",
    "    stocks_l = [research_stock_history(ticker) for ticker in tickers]\n",
    "    stocks_df = pd.concat(stocks_l, axis=1)\n",
    "    stocks_df = stocks_df.loc[:, stocks_df.max().sort_values(ascending=False).index] # Sort biggest ticker first\n",
    "    return stocks_df\n",
    "\n",
    "\n",
    "def plot_stocks(stocks_df, history=\"quarter\", dev_mode=False):\n",
    "    \"\"\"Create a plot for stock prices.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    stocks_df (DataFrame): Previous month's prices, with each stock as a column and mon-day (str) as index\n",
    "    history (str): How long in the past to plot. \"quarter\", \"month\", \"week\"\n",
    "    dev_mode (bool): If we're in dev/debug, output the plots to local files too.\n",
    "    \n",
    "    RETURNS\n",
    "    png_b64 (str): The PNG image as base64\n",
    "\n",
    "    \"\"\"\n",
    "    if history==\"quarter\":\n",
    "        # Tick for every 30 days, and ensure we include last day. Set de-dups if necessary\n",
    "        ticks = pd.Index(\n",
    "            set(\n",
    "                list(stocks_df.index[::30])[0:-1] + [stocks_df.index[-1]]\n",
    "            )\n",
    "        )\n",
    "    elif history==\"month\":\n",
    "        stocks_df = stocks_df.tail(30)\n",
    "        # Tick for every week, and ensure we include last day. Set de-dups if necessary                  \n",
    "        ticks = pd.Index(\n",
    "            set(\n",
    "                list(stocks_df.index[::7])[0:-1] + [stocks_df.index[-1]]\n",
    "            )\n",
    "        ) \n",
    "    elif history==\"week\":\n",
    "        stocks_df = stocks_df.tail(7)\n",
    "        # A tick for every day\n",
    "        ticks = list(stocks_df.index) \n",
    "    else:\n",
    "        logging.warning(f\"Unexpected value of `history` in plot_stocks(): {history}\")\n",
    "        return None\n",
    "\n",
    "    fig = plt.figure(figsize=(8,5))\n",
    "    plt.style.use(\"dark_background\")\n",
    "    sns.lineplot(data=stocks_df, palette=\"husl\", dashes=False, lw=4)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plot_max_y = stocks_df.iloc[:, 0].max() # Max of biggest ticker\n",
    "    _ = plt.ylim(0, 1.2 * plot_max_y) # Max of biggest ticker + 20%\n",
    "\n",
    "    # Plot tick marks\n",
    "    plt.xticks(\n",
    "        ticks, # Set() de-dups if necessary\n",
    "        rotation=45,\n",
    "        horizontalalignment='right',\n",
    "        fontweight='light'\n",
    "    )\n",
    "    ax = plt.gca() # Get current axis\n",
    "\n",
    "    # Add text labels\n",
    "    for stock_i, stock_name in enumerate(stocks_df.columns):\n",
    "        ticker_s = stocks_df[stock_name].dropna()\n",
    "\n",
    "        # Add stock name\n",
    "        ax.annotate(\n",
    "            xy=(ticker_s.index[-1], ticker_s.iloc[-1]),\n",
    "            xytext=(30,-5),\n",
    "            textcoords='offset points',\n",
    "            text=ticker_s.name, # The name of the Series = full name of stock, else ticker\n",
    "            fontsize=20,\n",
    "            color=ax.lines[stock_i].get_color(),\n",
    "            ha='left',\n",
    "        )\n",
    "        \n",
    "        # Add data labels\n",
    "        for i in [0, -1]:\n",
    "            ax.annotate(\n",
    "                xy=(ticker_s.index[i], ticker_s.iloc[i]),\n",
    "                xytext=(0, 20), # Place text 20 points above each data point\n",
    "                textcoords='offset points',\n",
    "                text=int(round(ticker_s.iloc[i],0)),\n",
    "                fontsize=18 if i ==-1 else 14,\n",
    "                color=ax.lines[stock_i].get_color(),\n",
    "                ha='center',\n",
    "                va='top'\n",
    "            )\n",
    "\n",
    "    plt.legend([],[], frameon=False) # Remove legend\n",
    "    ax.set_xlabel(None) # Remove \"Date\" name of X axis\n",
    "    \n",
    "    # Get raw image\n",
    "    png_bytes = BytesIO()\n",
    "    plt.savefig(png_bytes, format = \"png\", bbox_inches='tight')\n",
    "    png_bytes.seek(0)\n",
    "    \n",
    "    if dev_mode:\n",
    "        plt.savefig(f\"stocks_{'_'.join(stocks_df.columns)}.png\", format = \"png\", bbox_inches='tight')\n",
    "\n",
    "    plt.close(fig)\n",
    "    del fig\n",
    "    \n",
    "    return base64.b64encode(png_bytes.read()).decode()\n",
    "\n",
    "\n",
    "def get_stocks_plot(tickers, section_frequency=\"monthly\", dev_mode=False):\n",
    "    \"\"\"Get on stocks data for the issue.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    tickers (list of str): The abbreviation (ticker) of each stock  \n",
    "    section_frequency (str): How often we are reporting stocks in issue. Determines how far in the past to plot.\n",
    "    dev_mode (bool): If we're in dev/debug, output the plots to local files too.\n",
    "\n",
    "    RETURNS\n",
    "    stocks_plot (base64): Image for a single plot of tickers\n",
    "    \"\"\"\n",
    "    # Map how often we deliver this plot to how much historical data (context) to include in the graph\n",
    "    if section_frequency == \"monthly\":\n",
    "        history = \"quarter\"\n",
    "    elif section_frequency in [\"every_other_week\"]:\n",
    "        history = \"month\"\n",
    "    elif section_frequency in [\"daily\", \"weekdays\", \"weekly\"]:\n",
    "        history = \"week\"\n",
    "    else:\n",
    "        logging.warning(f\"Unexpected section_frequency in get_stocks_plot(): {tickers}, {section_frequency}\")\n",
    "        return None\n",
    "    \n",
    "    stocks_df = research_stock_histories(tickers)\n",
    "    return plot_stocks(stocks_df, history, dev_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6440ae-07e8-4dd6-8b16-c19ab34b4ae8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda14b97-ecdb-44b3-aff1-f75c7a8e9130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tag_class(element, soup, config):\n",
    "    \"\"\"Helper function to locate an HTML element's content by class and parse into a string.\n",
    "\n",
    "    ARGUMENT\n",
    "    element (str): Internal Finite News name of the element\n",
    "    soup (BeautifulSoup object): The parsed HTML to search\n",
    "    config (dict): The calendar_config dictionary describing the web calendar and how we'll process it \n",
    "\n",
    "    RETURNS\n",
    "    element_str (str): The text of the desired element, if present in the soup\n",
    "    \"\"\"\n",
    "    \n",
    "    class_name = f\"{element}_class\"\n",
    "    if class_name in config:\n",
    "        return (\n",
    "            soup\n",
    "            .find(class_=config[class_name])\n",
    "            .text\n",
    "            .strip()\n",
    "        )\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_event_details(event_soup, calendar_config):\n",
    "    \"\"\"Parse an event description from HTML to structured data.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    event_soup (BeautifulSoup object): Parsed HTML for the event\n",
    "    calendar_config (dict): Description of the website, calendar structure, and configuration\n",
    "\n",
    "    RETURNS\n",
    "    event (dict): Description of event with keys required for rendering in issue\n",
    "    \"\"\"\n",
    "    \n",
    "    event = {}\n",
    "    \n",
    "    # Extract text descriptions about the event\n",
    "    for element in [\"title\", \"venue\", \"dates\", \"description\"]:\n",
    "        event[element] = extract_tag_class(element, event_soup, calendar_config)\n",
    "    \n",
    "    # Extract thumbnail image\n",
    "    if \"image_html_class\" in calendar_config:\n",
    "        event[\"image_html\"] = event_soup.find(class_=calendar_config[\"image_html_class\"])\n",
    "        if \"placeholder_image_src\" in calendar_config:\n",
    "            if calendar_config[\"placeholder_image_src\"] in event[\"image_html\"].get(\"src\", \"\"):\n",
    "                event[\"image_html\"] = calendar_config[\"placeholder_image_replacement_url\"]\n",
    "    else:\n",
    "        event[\"image_html\"] = \"\"\n",
    "\n",
    "    # Extract link   \n",
    "    if \"link_url_class\" in calendar_config and \"link_url_child_key\" in \"calendar_config\":\n",
    "        event[\"link_url\"] = (\n",
    "            event_soup\n",
    "            .find(class_=calendar_config[\"link_url_class\"])\n",
    "            .get(calendar_config[\"link_url_child_key\"], \"\")\n",
    "        )\n",
    "    else:\n",
    "        event[\"link_url\"] = \"\"\n",
    "        \n",
    "    return event\n",
    "\n",
    "\n",
    "def scrape_calendar_page(url_base, page, event_item_tag, event_list_class, requests_timeout):\n",
    "    \"\"\"Pull content from one page of a web calendar.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    url_base (str): The url for the calendar, with {PAGE} as a placeholder\n",
    "    page (int): The page to request\n",
    "    event_item_tag (str): The HTML tag where each event is stored\n",
    "    event_list_class (str): The element CSS class for those event tags\n",
    "\n",
    "\n",
    "    RETURNS\n",
    "    page_soup (BeautifulSoup object): Parsed HTML for the calendar page\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        url = url_base.replace(\"{PAGE}\", str(page))\n",
    "        response = requests.get(url, timeout=requests_timeout)\n",
    "        return (\n",
    "            BeautifulSoup(response.text, \"html.parser\")\n",
    "            .find_all(event_item_tag, class_=event_list_class)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"scrape_calendar_page: {str(type(e))}, {str(e)}. {url}\")\n",
    "\n",
    "\n",
    "def scrape_calendar(calendar_config, requests_timeout):\n",
    "    \"\"\"Pull content from a web calendar. Handle multi-page calendars.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    calendar_config (dict): Description of the website, calendar structure, and configuration\n",
    "    requests_timeout (int): Number of seconds to wait before giving up on an HTTP request\n",
    "\n",
    "    RETURNS\n",
    "    calendar_events (lsit of dict): List of event descriptions\n",
    "    \"\"\"\n",
    "    \n",
    "    today = datetime.today() \n",
    "    start_date = today.strftime('%m-%d-%Y')\n",
    "    end_date = (\n",
    "        (today + timedelta(days=calendar_config[\"window\"]))\n",
    "        .strftime('%m-%d-%Y')\n",
    "    )\n",
    "    url_base = (\n",
    "        calendar_config[\"url_base\"]\n",
    "        .replace(\"{START_DATE}\", start_date)\n",
    "        .replace(\"{END_DATE}\", end_date)\n",
    "    )\n",
    "\n",
    "    exhausted = False\n",
    "    calendar_events = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        page_soup = scrape_calendar_page(\n",
    "            url_base,\n",
    "            page,\n",
    "            calendar_config[\"event_item_tag\"],\n",
    "            calendar_config[\"event_list_class\"],\n",
    "            requests_timeout\n",
    "        )\n",
    "        if page_soup:\n",
    "            page_events = [extract_event_details(event_soup, calendar_config) for event_soup in page_soup]\n",
    "            calendar_events.append(page_events)\n",
    "            page += 1\n",
    "        else:\n",
    "            return [item for sublist in calendar_events for item in sublist] # FLatten nested list\n",
    "\n",
    "        \n",
    "def format_event(event):\n",
    "    \"\"\"Render one event as a table row\n",
    "    \n",
    "    ARGUMENT\n",
    "    event (dict): Description of event\n",
    "    \n",
    "    RETURNS\n",
    "    event_row (str): HTML table row describing that event\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(event['title'])<2:\n",
    "        return ''\n",
    "    return f\"\"\"\n",
    "    <tr>\n",
    "       <td>\n",
    "           {event['image_html']}\n",
    "       </td>\n",
    "       <td>\n",
    "           <h4><a href=\"{event['link_url']}\">{event['title']}</a></h4>\n",
    "           <p><b>{event['venue']}</b></p>\n",
    "           <p><b><i>{event['dates']}</b></i></p>\n",
    "           <p>{event['description']}</p>\n",
    "           <br>\n",
    "        </td>\n",
    "    </tr>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_calendar_events(calendar_config, requests_timeout):\n",
    "    \"\"\"Pull all events from a website calendar, formatting results as HTML table.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    calendar_config (dict): Description of the website, calendar structure, and configuration\n",
    "    requests_timeout (int): Number of seconds to wait before giving up on an HTTP request\n",
    "    \n",
    "    RETURNS\n",
    "    calendar_html (str): List of events formatted as an HTML table\n",
    "    \"\"\"\n",
    "    \n",
    "    calendar_events = scrape_calendar(calendar_config, requests_timeout)\n",
    "\n",
    "    # Limit total events if requested\n",
    "    if calendar_config.get(\"max_events\"):\n",
    "        calendar_events = calendar_events[:min(calendar_config[\"max_events\"], len(calendar_events))]\n",
    "    return f\"\"\"\n",
    "                <table>\n",
    "                    {''.join([format_event(event) for event in calendar_events])}\n",
    "                </table>\n",
    "            \"\"\".replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd84b9b-d17d-49d2-96c8-0d2f7253a4ff",
   "metadata": {},
   "source": [
    "### Misc sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7380e490-920d-43fd-a809-82f1385f1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mbta_alerts(route, station_ids, direction_id, requests_timeout):\n",
    "    \"\"\"Use the MBTA API to get alerts for a station.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    route (str): The mbta id of the route. Browse at https://api-v3.mbta.com/routes\n",
    "    station_ids (list of str): The mbta ids of the station, either parent station ID from https://api-v3.mbta.com/stops, or get the end of the URL like https://www.mbta.com/stops/place-sstat\n",
    "    direction_id (int): 0 for outbound, 1 for inbound\n",
    "    requests_timeout (int): Number of seconds to wait before giving up on an HTTP request\n",
    "    \n",
    "    RETURNS\n",
    "    alerts (list of str): Alerts for that station\n",
    "    \n",
    "    \"\"\"\n",
    "    if not route or not station_ids:\n",
    "        return []\n",
    "    url = f\"https://api-v3.mbta.com/alerts?filter[route]={route}&filter[stop]={','.join(station_ids)}&filter[direction_id]={direction_id}\"\n",
    "    response = requests.get(url, timeout=requests_timeout)\n",
    "    return [\n",
    "        f\"üöÇ MBTA ruh-roh: {alert['attributes']['header'].strip()}\"\n",
    "        for alert in response.json()[\"data\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_car_talk_credit(bucket_path):\n",
    "    \"\"\"Pull a random Car Talk credit from a CSV on S3. \n",
    "    \n",
    "    NOTE\n",
    "    - These credits are fake staff credits that were used at the end of each episode of\n",
    "    the National Public Radio automotive advice radio show, Car Talk\n",
    "    - They came from downloading https://www.cartalk.com/content/staff-credits.\n",
    "\n",
    "    ARGUMENTS \n",
    "    bucket_path (str): The location of the S3 bucket where required files are stored.\n",
    "\n",
    "    RETURNS\n",
    "    car_talk_credit (str): A fake staff member to thank for creation of this issue of Finite News :D\n",
    "    \"\"\"\n",
    "    \n",
    "    return \": \".join(\n",
    "        pd.read_csv(bucket_path + \"car_talk_credits.csv\", header=None)\n",
    "        .sample(1)\n",
    "        .values\n",
    "        .flatten()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "\n",
    "def get_screenshots(sources):\n",
    "    \"\"\"Not currently working on SM. Disabled.\"\"\"\n",
    "    return []\n",
    "#     options = Options()\n",
    "#     options.add_argument('headless')\n",
    "#     s=Service(ChromeDriverManager().install())\n",
    "#     driver = webdriver.Chrome(service=s, options=options)\n",
    "#     driver.maximize_window()\n",
    "\n",
    "#     screenshots = []\n",
    "#     for source in sources:\n",
    "#         url = source[\"url\"]\n",
    "#         driver.get(url)\n",
    "#         try:\n",
    "#             elements = driver.find_elements(By.CLASS_NAME, source[\"element_class\"])\n",
    "#             if source.get(\"automate_gradually\", False):\n",
    "#             # TODO: Temporary workaround for Birdcast. There's surely a better way\n",
    "#                 b64_screenshots = [element.screenshot_as_base64 for element in elements]\n",
    "#                 screenshot_b64 = b64_screenshots[source[\"element_number\"]]\n",
    "#             else:       \n",
    "#                 # The simpler way that should work for nondynamically loaded images\n",
    "#                 chart_element = elements[source[\"element_number\"]]\n",
    "#                 screenshot_b64 = chart_element.screenshot_as_base64\n",
    "#         except Exception as e:\n",
    "#             logging.warning(f\"Selenium error on {source['url']}: {str(type(e))}, {str(e)}\")\n",
    "#         screenshots.append(screenshot_b64)\n",
    "#         driver.quit()\n",
    "#     return screenshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c04717-5c9c-4642-b14f-01fcb6dfab8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ‚úÇÔ∏è Edit\n",
    "Refine the news and other reporting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71e6ef-d792-49f6-a5d9-ead256585180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    \"\"\"Utility function that removes all emojis from a string\n",
    "    ARGUMENTS\n",
    "    text (str): A string\n",
    "\n",
    "    RETURNS\n",
    "    text without emojis (or new whitespace created by removing emojis)\n",
    "\n",
    "    \"\"\"\n",
    "    return (\n",
    "        emoji.replace_emoji(text, replace=\"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "\n",
    "def cache_issue_content(content, bucket_path, cache_path):\n",
    "    \"\"\"Export a list of this issue's headlines and other content that we don't want to show again in the next issue.\n",
    "    \n",
    "    NOTE: Must call this before edit_research so we carry forward repeats that were dropped too\n",
    "\n",
    "    ARGUMENTS\n",
    "    content (list of str): Headlines or other content items that shouldn't be repeated in subsequent issues\n",
    "    bucket_path (str): The location of the S3 bucket where required files are stored.\n",
    "    cache_path (str): The path on the S3 bucket for this subscriber's cache of last issue's headlines\n",
    "\n",
    "    RETURNS\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    with fs.open(bucket_path + cache_path, \"w\") as cache_file:\n",
    "        for item in content:\n",
    "            cache_file.write(f\"{item}\\n\")\n",
    "        logging.info(f\"Wrote issue content to {bucket_path+cache_path}\")\n",
    "            \n",
    "\n",
    "def apply_one_headline_keyword_filter(headlines, keyword):\n",
    "    \"\"\"Limit the issue to a maximum of one headline that mentions this keyword.\n",
    "\n",
    "    ARGUMENTS\n",
    "    headlines (list of str): Headlines from all sources\n",
    "\n",
    "    RETURNS\n",
    "    new_headlines (list of str): Headlines except those that contain this keyword\n",
    "    \"\"\"\n",
    "    \n",
    "    new_headlines = []\n",
    "    kw_counter = 0\n",
    "    keyword = keyword.lower()\n",
    "    for headline in headlines:\n",
    "        has_kw = keyword in headline.lower() # Could add spaCy tokenizer, split on spaces, punctuation. But the benefit would be teeny. Empirically this has been working perfectly for months.\n",
    "        kw_counter += has_kw\n",
    "        if not has_kw or kw_counter<=1:\n",
    "            new_headlines.append(headline)\n",
    "    return new_headlines\n",
    "\n",
    "\n",
    "def remove_items_in_last_issue(new_items, bucket_path, cache_path):\n",
    "    \"\"\"Delete content that we already presented in the last issue.\n",
    "    \n",
    "    Ignores emojis in the comparison. That way a preface emoji (could be changed in publication_config) alone\n",
    "    wouldn't prevent a match with an identical headline in the cache (with the old preface) \n",
    "\n",
    "    TODO\n",
    "    Ignore the entire preface in this comparison. Even better, don't add the preface till after editing headlines.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    new_items (list of str): Fresh content\n",
    "    bucket_path (str): The location of the S3 bucket where required files are stored.\n",
    "    cache_path (str): The path on the S3 bucket for this subscriber's cache of the last issue's content\n",
    "    \n",
    "    RETURNS\n",
    "    fresh_items (list of str): Content from new_items that was not in the last issue\n",
    "    \"\"\"\n",
    "    last_issue_items = [remove_emojis(line) for line in load_s3(bucket_path, cache_path)]\n",
    "    fresh_items = [item for item in new_items if remove_emojis(item) not in last_issue_items]\n",
    "    logging.info(f\"Removed items that were in last issue: {[item for item in new_items if item in last_issue_items]}\") \n",
    "    return fresh_items\n",
    "\n",
    "\n",
    "def unnest_list(l_of_ls):\n",
    "    \"\"\"Extract headlines from all sources we researched.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    l_of_ls (list of lists)\n",
    "\n",
    "    RETURNS\n",
    "    l (list of str): Flat list of headlines retrieved from all sources\n",
    "    \n",
    "    \"\"\"\n",
    "    l_of_ls_rinsed = [l for l in l_of_ls if l] # Remove sublists that are None\n",
    "    return [item for sublist in l_of_ls_rinsed for item in sublist]\n",
    "\n",
    "\n",
    "def lower_list(l):\n",
    "    \"\"\"Helper function to lowercase the items in a list of strings.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    l (list of str): A list of headlines\n",
    "    \n",
    "    RETURNS\n",
    "    l_lower (list of str): A list of lowercase headlines\n",
    "    \"\"\"\n",
    "    \n",
    "    if not l:\n",
    "        return None\n",
    "    return [item.lower() for item in l]\n",
    "\n",
    "\n",
    "def breaks_rule(headline, cant_begin_with, cant_contain, cant_end_with):\n",
    "    \"\"\"Evaluate whether a headline breaks any of the passed sets of editorial rules\n",
    "    \n",
    "    ARGUMENTS\n",
    "    headline (str): The text to evaluate\n",
    "    cant_begin_with (list of str): Text that a headline cannot start with\n",
    "    cant_contain (list of str): Text that cannot exist anywhere in a headline\n",
    "    cant_end_with (list of str): Text that a headline cannot end with\n",
    "    \n",
    "    RETURNS\n",
    "    True if this headline violates any rule\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ignore emojis, which often preface the headline (and interfere with cant_begin_with\n",
    "    # TODO: Edit headlines before adding prefaces\n",
    "    headline_clean = remove_emojis(headline)\n",
    "    \n",
    "    for phrase in cant_begin_with:\n",
    "        if headline_clean.startswith(phrase):\n",
    "            return True\n",
    "    for phrase in cant_contain:\n",
    "        if phrase in headline_clean:\n",
    "            return True\n",
    "    for phrase in cant_end_with:\n",
    "        if headline_clean.endswith(phrase):\n",
    "            return True\n",
    "\n",
    "        \n",
    "def apply_substance_rules(headlines, substance_rules):\n",
    "    \"\"\"Remove headlines that fail our logic for ensuring a headline is substanative.\n",
    "\n",
    "    ARGUMENTS\n",
    "    headlines (list of str): The headlines retrieved from all sources\n",
    "    substance_rules (dict): The editorial rules, which consist of lists of phrases\n",
    "    \n",
    "    RETURNS\n",
    "    kept_headlines (list of str): The headlines that pass all substrance rules.\n",
    "\n",
    "    \"\"\"\n",
    "    cant_begin_with = lower_list(substance_rules.get(\"cant_begin_with\", []))\n",
    "    cant_contain = lower_list(substance_rules.get(\"cant_contain\", []))\n",
    "    cant_end_with = lower_list(substance_rules.get(\"cant_end_with\", []))\n",
    "    removed_headlines = [headline for headline in headlines if breaks_rule(headline.lower(), cant_begin_with, cant_contain, cant_end_with)]\n",
    "    logging.info(f\"Substance rules removed: {removed_headlines}\")\n",
    "    kept_headlines = [headline for headline in headlines if headline not in removed_headlines]\n",
    "    return kept_headlines \n",
    "\n",
    "\n",
    "def smart_dedup(headlines, smart_dedup_config, prefaces_to_ignore=[]):\n",
    "    \"\"\"Use semantic de-duping to avoid showing two headlines about the same news events, even if they use different words.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    headlines (list of str): The headlines from research\n",
    "    smart_dedup_config (dict): Publication's settings for using the smart deduplication\n",
    "    prefaces_to_ignore (list): A list of repeated prefaces that may appear at beginnings of headlines, if we want smart_dedup to ignore them when computing headline similarity.\n",
    "                               So we don't get high similarity just because two headlines start with \"üçª FiniteBrews: \" for example.\n",
    "\n",
    "    RETURNS\n",
    "    deduped_headlines (list of str): Headlines after de-duplication\n",
    "    \"\"\"\n",
    "    try:       \n",
    "        # Set things up\n",
    "        model = SentenceTransformer(smart_dedup_config[\"model\"])\n",
    "        # First, temporarily remove prefaces to headlines.\n",
    "        # TODO: Refactor so that FiniteNews doesn't add prefaces until after editing. Then we can avoid this hokey pokey move.\n",
    "        headlines_clean = headlines\n",
    "        for preface in set(prefaces_to_ignore):\n",
    "            headlines_clean = [h.replace(preface, \"\").strip() for h in headlines_clean]\n",
    "        logging.info(f\"Smart_deduper prefaces to ignore: {set(prefaces_to_ignore)}\")\n",
    "            \n",
    "        # Second, find pairs of headlines that are semantically similar\n",
    "        # We'll get their sentence embeddings and use cosine_similarity\n",
    "\n",
    "        embeddings = model.encode(headlines_clean, convert_to_tensor=True)\n",
    "        similarity_matrix = cos_sim(embeddings, embeddings)\n",
    "        \n",
    "        dups_found = [\n",
    "            # Get every unique combination of headlines...\n",
    "            [headlines[i], headlines[j]]\n",
    "            for i in range(embeddings.shape[0])\n",
    "            for j in range(embeddings.shape[0])\n",
    "            # ...if their semanatic similarity meets threshold\n",
    "            if similarity_matrix[i,j] >= smart_dedup_config[\"threshold\"]\n",
    "            # ...and a headlines isn't being compared to itself\n",
    "            and i!=j\n",
    "        ]\n",
    "\n",
    "        if not dups_found:\n",
    "            logging.info(f\"Smart deduper: no semantic dups found\")\n",
    "            return headlines\n",
    "\n",
    "        # Third, apply the transitive property of headline similarity! :D\n",
    "        # Given pairs of headlines flagged as semantically similar, find the minimum set of unique items.\n",
    "        # We assume if (headline A similar to headline B) and (B similar to C), we keep A and drop B and C\n",
    "        keepers = {dups_found[0][0]} # Initialize by de-duplicating first pair of items. Keep first in pair, drop second\n",
    "        droppers = {dups_found[0][1]}\n",
    "        for pair in dups_found[1:]: # Then walk through the rest of the pairs\n",
    "            # Have we already flagged at least one item in the pair as a keeper or a dropper?\n",
    "            if set(pair).intersection(keepers.union(droppers)):\n",
    "                # Find the unseen item(s) and drop it (them). \n",
    "                # By transitive property, it's similar to a seen item so we won't keep it.\n",
    "                if pair[0] not in droppers and pair[0] not in keepers: \n",
    "                    droppers.add(pair[0])\n",
    "                if pair[1] not in droppers and pair[1] not in keepers: \n",
    "                    droppers.add(pair[1])\n",
    "            # Have we never seen either item in the new pair before?\n",
    "            else:\n",
    "                # Keep the first, drop the second\n",
    "                keepers.add(pair[0])\n",
    "                droppers.add(pair[1])\n",
    "\n",
    "        # Finally, map the headlines to drop to the full headline including preface.\n",
    "        droppers = [h for h in headlines for d in droppers if d in h] # Droppers won't change if prefaces_to_ignore is empty.\n",
    "            \n",
    "        logging.info(f\"Smart dededuper found the following pairs of headlines that met threshold: {dups_found}\")\n",
    "        logging.info(f\"Smart deduper kept: {keepers}. Removed: {droppers}\")\n",
    "        return [h for h in headlines if h not in droppers]\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Smart deduper failed: {str(type(e))}, {str(e)}\")\n",
    "        return headlines\n",
    "\n",
    "    \n",
    "def openai_chat_completion(gpt_config, message):\n",
    "    \"\"\"Make an API call to the OpenAI GPT chat endpoint.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    gpt_config (dict): Parameters for using the API\n",
    "    message (str): The full prompt to send GPT, including generic lead-in, headlines, and instruction (customized to each subscriber)\n",
    "    \n",
    "    RETURNS\n",
    "    headlines_to_remove_str (string): GPT's response of which headlines to remove, in str format\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=gpt_config[\"substance_filter_model\"],\n",
    "        messages=[\n",
    "            {\"role\":\"system\", \"content\": gpt_config[\"system_role\"]},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ],\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def apply_substance_filter_model(headlines, gpt_config):\n",
    "    \"\"\"Use LLM to remove headlines that don't say much useful.\n",
    "    \n",
    "    NOTE\n",
    "    Requires an OPENAI_API_KEY in AWS Secrets Manager.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    headlines (list): List of string headlines, original candidates for the issue\n",
    "    gpt_config (dict): Configuration for editing headlines using GPT LLM through the Open AI API.\n",
    "    \n",
    "    RETURNS\n",
    "    kept_headlines (list): The headlines that GPT did not remove\n",
    "    \"\"\"\n",
    "    \n",
    "    GPT_RETRY_SLEEP = 30\n",
    "    openai.api_key = get_fn_secret(\"OPENAI_API_KEY\")\n",
    "    headlines_for_gpt = [f\"* {headline}\" for headline in headlines]\n",
    "    lead_in = \"Here are today's news headlines:\"\n",
    "    message = lead_in + \"\\n\" + \"\\n\".join(headlines_for_gpt) + \"\\n\" + gpt_config[\"instruction\"]\n",
    "    try:\n",
    "        try:\n",
    "            headlines_to_remove_str = openai_chat_completion(gpt_config, message)\n",
    "        except openai.error.APIConnectionError:\n",
    "            logging.info(f\"OpenAI API error. Waiting {GPT_RETRY_SLEEP} secs, retrying...\")\n",
    "            sleep(GPT_RETRY_SLEEP)\n",
    "            headlines_to_remove_str = openai_chat_completion(gpt_config, message)\n",
    "            logging.info(f\"OpenAI API error. Waiting {GPT_RETRY_SLEEP} secs, retrying...\")\n",
    "            logging.info(\"Retry worked! üòÖ\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"OpenAI failed: {str(type(e))}, {str(e)}\")\n",
    "        headlines_to_remove_str = None\n",
    "\n",
    "    headlines_to_remove = [h for h in headlines_to_remove_str.split(\"\\n\")]\n",
    "    removed_headlines = [headline for headline in headlines if headline in headlines_to_remove] # Extra QC step to make sure GPT didn't return a hallucination that wasn't in headlines we sent it.\n",
    "    logging.warning(f\"GPT removed: {removed_headlines}\") \n",
    "    return [headline for headline in headlines if headline not in removed_headlines]\n",
    "\n",
    "\n",
    "def clean_headline(headline, enforce_trailing_period=True):\n",
    "    \"\"\"Standardize text formatting of a headline string\n",
    "    \n",
    "    NOTE\n",
    "    - Assumes we have already stripped white space from beginning and end of headline\n",
    "    - We apply these steps before applying substance rules, which rely on standard format,\n",
    "    before checking if these headlines were in the last issue, and before caching this issue's headlines.\n",
    "\n",
    "    ARGUMENTS\n",
    "    headline (str): A single headline.\n",
    "    enforce_trailing_period (bool): Whether to ensure headlines end in a period. True for news and alerts. False for image sections.\n",
    "    \n",
    "    RETURNS\n",
    "    headline (str): A single, clean headline.\n",
    "    \"\"\" \n",
    "    \n",
    "    headline = (\n",
    "        headline\n",
    "        .replace(\"‚Äô\",\"'\") # Standardize apostrophe characters\n",
    "        .replace(\"‚Äò\",\"'\")\n",
    "        .replace(\"\\xa0\", \" \") # Non-breaking space unicode\n",
    "    )\n",
    "    if enforce_trailing_period:\n",
    "        headline = headline + \".\" if not headline.endswith(\".\") and not (headline.endswith(\"?\") or headline.endswith(\"!\")) else headline  # Ensure all have trailing period\n",
    "    return headline\n",
    "\n",
    "\n",
    "def edit_headlines(raw_headlines,\n",
    "                   issue_config,\n",
    "                   filter_for_substance=True,\n",
    "                   smart_deduplicate=True,\n",
    "                   enforce_trailing_period=True,\n",
    "                   sources_type=\"news_sources\"):\n",
    "    \"\"\"Apply all editorial policies to the headlines.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    raw_headlines (list): List of string headlines, original candidates for the issue\n",
    "    issue_config (dict): The settings for the issue\n",
    "    filter_for_substance (bool): Apply rules ¬± LLM to remove non-substantive headlines\n",
    "    smart_deduplicate (bool): When headlines have similar meaning, only keep one in the set.\n",
    "    enforce_trailing_period (bool): Whether to ensure headlines end in a period. True for news and alerts. False for image sections.\n",
    "    sources_type (str): The name of the key in issue_config to get the lists of sources for these headlines, so we can find their prefaces used.\n",
    "    \n",
    "    RETURNS\n",
    "    edited_headlines (list): Headlines after filtering ones that violate editorial policies\n",
    "    \"\"\"\n",
    "    \n",
    "    if not raw_headlines:\n",
    "        return raw_headlines\n",
    "    # Apply deterministic cleaning first\n",
    "    edited_headlines = remove_items_in_last_issue(raw_headlines, issue_config[\"bucket_path\"], issue_config[\"editorial\"][\"cache_path\"])\n",
    "    edited_headlines = [clean_headline(headline, enforce_trailing_period) for headline in edited_headlines] # Do after removing repeats, since we cache the raw uncleaned\n",
    "    for keyword in issue_config[\"editorial\"][\"one_headline_keywords\"]:\n",
    "        edited_headlines = apply_one_headline_keyword_filter(edited_headlines, keyword) # No list comprehension because each cycle can change edited_headlines\n",
    "    if edited_headlines and filter_for_substance:\n",
    "        edited_headlines = apply_substance_rules(edited_headlines, issue_config[\"editorial\"][\"substance_rules\"])\n",
    "    # Then probablistic LLM cleaning\n",
    "    # Start with substance filter model. Remove ones that aren't great headlines.\n",
    "        if issue_config[\"editorial\"][\"gpt\"]:\n",
    "            edited_headlines = apply_substance_filter_model(edited_headlines, issue_config[\"editorial\"][\"gpt\"])\n",
    "        else:\n",
    "            logging.info(\"Did not apply LLM substance model. GPT not configured.\")\n",
    "    # Finally, smart deduplicate (by semantic similarity) the remaining headlines that are individually fine options.\n",
    "    if edited_headlines and smart_deduplicate and issue_config[\"editorial\"][\"smart_deduper\"]:\n",
    "        prefaces_to_ignore = [source.get('preface', None) for source in issue_config[sources_type]]\n",
    "        prefaces_to_ignore = [p for p in prefaces_to_ignore if p]\n",
    "        edited_headlines = smart_dedup(\n",
    "            edited_headlines,\n",
    "            issue_config[\"editorial\"][\"smart_deduper\"],\n",
    "            prefaces_to_ignore\n",
    "        )\n",
    "    logging.info(\"Edited headlines: \" + str(edited_headlines))\n",
    "    return edited_headlines\n",
    "\n",
    "\n",
    "def edit_sports_headlines(headlines, teams):\n",
    "    \"\"\"Clean and harmonize game headlines. The key outcome: when two of our tracked teams are playing each other, only report once, not twice.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    headlines (list of str): News related to today's game(s) for tracked teams\n",
    "    teams (list of str): The names of the tracked teams that may be in the headlines.\n",
    "    \n",
    "    RETURNS\n",
    "    cleaned_headlines (list of str): Harmonized news about today's game(s)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Avoid [None] lists\n",
    "    headlines = [h for h in headlines if h] \n",
    "\n",
    "    # If two tracked teams are playing each other, only give one headline\n",
    "    cleaned_headlines = []\n",
    "    teams_already_reported = set()\n",
    "    for headline in headlines:\n",
    "        teams_found = {t for t in teams if t in headline}\n",
    "        if not teams_already_reported.intersection(teams_found):\n",
    "            cleaned_headlines.append(headline)\n",
    "        teams_already_reported.update(teams_found) \n",
    "    return cleaned_headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae149e5-00b7-4a3e-9bf0-5cf467752901",
   "metadata": {},
   "source": [
    "## üé® Design\n",
    "Lay out the email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec37521-c650-4e2b-ae4d-2dbd40446ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_emoji(forecast):\n",
    "    \"\"\"\n",
    "    Label a weather forecast with an emoji. It's used to spice up the section header.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    forecast (dict): Attributes of the forecast retrieved.\n",
    "    \n",
    "    RETURNS\n",
    "    emoji (str): One character\n",
    "    \"\"\"\n",
    "\n",
    "    forecast = forecast.lower()\n",
    "    if \"tornado\" in forecast:\n",
    "        return \"üå™Ô∏è\"\n",
    "    if \"hurricane\" in forecast:\n",
    "        return \"üåÄ\"\n",
    "    if \"thunder\" in forecast or \"lightning\" in forecast:\n",
    "        return \"‚ö°\"\n",
    "    if \"snow\" in forecast or \"flurries\" in forecast:\n",
    "        return \"‚ùÑÔ∏è\"\n",
    "    if \"rain\" in forecast or \"pour\" in forecast or \"shower\" in forecast or \"drizzle\" in forecast: # Must come after snow for snow showers\n",
    "        return \"‚òî\"\n",
    "    if \"hot\" in forecast:\n",
    "        return \"ü•µ\"\n",
    "    if \"freezing\" in forecast:\n",
    "        return \"ü•∂\"\n",
    "    if \"partly cloudy\" in forecast or \"mostly sunny\" in forecast:\n",
    "        return \"üå§Ô∏è\"\n",
    "    if \"sunny\" in forecast or \"beautiful\" in forecast or \"warm\" in forecast: # Must come after mostly sunny\n",
    "        return \"üòé\"\n",
    "    if \"mostly cloudy\" in forecast:\n",
    "        return \"üå•Ô∏è\"\n",
    "    if \"cloudy\" in forecast:\n",
    "        return \"‚òÅÔ∏è\"\n",
    "    if \"windy\" in forecast:\n",
    "        return \"üå¨Ô∏è\"\n",
    "    if \"fog\" in forecast or \"smoke\" in forecast:\n",
    "        return \"üò∂‚Äçüå´Ô∏è\"\n",
    "    return \"üîÆ\"\n",
    "\n",
    "\n",
    "def populate_template(template_text, placeholder, new_content, html_list=None, condition=True):\n",
    "    \"\"\"Replaces placeholder text from a template with real content. \n",
    "    If there's no content, remove the placeholder.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    template_text (str): The text from a template we are filling in.\n",
    "    placeholder (str): The characters in the template where content goes\n",
    "    new_content (str): The content to add to the template, or None\n",
    "    html_list (list): Optional, appends a list of items to end of `replacement` as html list. If this argument is passed, and list is empty, we remove placheolder altogether.\n",
    "    condition (object, bool, or None): Optional, if False/None we replace placeholder with \"\" regardless of `new_content`. Usually this means some _piece_ of new_content must be non-Null, or we should remove the entire section\n",
    "    \n",
    "    RETURNS\n",
    "    populated_text (str): The text with placeholder filled in or removed\n",
    "    \"\"\"\n",
    "    # If placeholder doesn't exist in template, log that nothing will get populated below\n",
    "    if placeholder not in template_text: \n",
    "        logging.warning(f\"Template does not contain section so we cannot populate it. Placeholder: {placeholder}. Content to populate: {new_content}. Template text state: {template_text}\")\n",
    "\n",
    "    if not new_content or not condition: # This also checks if new_content is None (vs \"\")\n",
    "        replacement = \"\"\n",
    "    elif type(html_list)==list:     # If placeholder is populated by a list of items, insert that HTML into `replacement` string\n",
    "        if len(html_list)>0:\n",
    "            replacement = new_content + \"<ul>\" + ''.join([f'<li>{item}</li>' for item in html_list]) + \"</ul>\"\n",
    "        else:\n",
    "            replacement = \"\"\n",
    "    else:\n",
    "        replacement = new_content\n",
    "\n",
    "    return template_text.replace(placeholder, replacement)\n",
    "\n",
    "\n",
    "def format_issue(\n",
    "    issue_config,\n",
    "    content,\n",
    "    log_stream=None\n",
    "):\n",
    "    \"\"\"Organize the final content as HTML for one subscriber's issue.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    content (dict): The content to go into an issue, with these keys (although their values may be None/[]):\n",
    "        - headlines (list of str): The final news headlines to be reported in this issue\n",
    "        - forecast (dict): Forecast content, if any\n",
    "        - events_html (str): HTML-formatted section with upcoming events\n",
    "        - stock_plots (list of base64): List of pngs as base64\n",
    "        - screenshots (list): Other images to attach to the image\n",
    "    issue_config (dict): The settings for the issue\n",
    "    log_stream (String IO): Optional, the log report from running Finite News\n",
    "    \n",
    "    RETURNS\n",
    "    html (str): The Finite News template populated with the final content\n",
    "    \"\"\"\n",
    "        \n",
    "    html = issue_config[\"layout\"][\"template_html\"]\n",
    "    \n",
    "    html = populate_template(html, \"[[LOGO_URL]]\", issue_config[\"layout\"][\"logo_url\"])\n",
    "    html = populate_template(html, \"[[SLOGAN]]\", choice(issue_config.get(\"slogans\", [\"\"])))\n",
    "    html = populate_template(html, \"[[HEADLINES_BLOCK]]\", \"<h3>üóûÔ∏è News</h3>\", content[\"headlines\"])\n",
    "    html = populate_template(html, \"[[ALERTS_BLOCK]]\", \"<h3>üö® Alert weeoooweeooo</h3>\", content[\"alerts\"])\n",
    "\n",
    "    if content[\"forecast\"]:\n",
    "        weather_emoji = get_weather_emoji(content[\"forecast\"][\"short\"])\n",
    "        weather_icon = f\"<img src={content['forecast']['icon_url']} alt='Forecast icon'><br>\" if \"icon_url\" in content[\"forecast\"] else \"\"\n",
    "        weather_block = f\"<h3>{weather_emoji} {content['forecast']['short']}</h3>{weather_icon}<p>{content['forecast']['detailed']}</p>\"\n",
    "    else:\n",
    "        weather_block = \"\"\n",
    "    html = populate_template(html, \"[[WEATHER_BLOCK]]\", weather_block)\n",
    "\n",
    "    html = populate_template(html, \"[[EVENTS_BLOCK]]\", f\"<h3>ü™© Upcoming events</h3>{content['events_html']}\", condition=content[\"events_html\"])\n",
    "    stocks_block = \"\".join([f\"<img src='cid:image_{i}', alt='image_{i}'><br>\" for i in range(0,len(content[\"stock_plots\"]))]) # Reference cids of images attached to email\n",
    "    html = populate_template(html, \"[[STOCKS_BLOCK]]\", f\"<h3>üí∞ Financial update</h3>{stocks_block}\", condition=stocks_block)\n",
    "    images_block = \"\".join([f\"<img src='cid:image_{i + len(content['stock_plots'])}', alt='image_{i + len(content['stock_plots'])}'><br>\" for i in range(0,len(content['images']) - len(content['stock_plots']) )]) # Increment cids if stock plots already attached to email\n",
    "    image_urls_block = ''.join(content[\"image_urls\"])\n",
    "    html = populate_template(html, \"[[IMAGES_BLOCK]]\", f\"<h3>üì∏ Finstagram</h3>{images_block}{image_urls_block}\", condition=images_block+image_urls_block)\n",
    "\n",
    "    try:\n",
    "        thoughts = issue_config[\"thoughts_of_the_day\"]\n",
    "        if len(thoughts)==0:\n",
    "            thoughts = [None] # To make choice() happy, it can't handle []\n",
    "        html = populate_template(html, \"[[THOUGHT_OF_THE_DAY]]\", f\"\"\"<h3>üí≠ Thought for the day</h3><p>{choice(thoughts)}</p>\"\"\", condition=len(issue_config[\"thoughts_of_the_day\"])>0)\n",
    "    except TypeError as e:\n",
    "        logging.warning(f\"TypeError on replace closing thoughts. Yaml malfored?: {e}. thoughts_of_the_day type: {type(issue_config['thoughts_of_the_day'])}. Expected string. {issue_config['thoughts_of_the_day']}\")\n",
    "        html = populate_template(html, \"[[THOUGHT_OF_THE_DAY]]\",\"\")\n",
    "\n",
    "    # Credits section\n",
    "    car_talk_block = \"<p>\" + get_car_talk_credit(issue_config[\"bucket_path\"]) + \"</p><br>\" if issue_config[\"editorial\"][\"add_car_talk_credit\"] else \"\" # We need this car_talk_block variable later too\n",
    "    html = populate_template(html, \"[[CAR_TALK_CREDIT]]\", car_talk_block)\n",
    "    attributions=get_attributions(\n",
    "        general_sources=issue_config[\"news_sources\"] + issue_config[\"events_sources\"] + issue_config[\"alerts_sources\"] + issue_config[\"image_sources\"] ,\n",
    "        sports_tracked=issue_config[\"sports\"],\n",
    "        weather_source=issue_config[\"forecast\"].get(\"source\", None),\n",
    "        stocks_used=len(content[\"stock_plots\"])>0,\n",
    "        car_talk_used=issue_config[\"editorial\"][\"add_car_talk_credit\"]\n",
    "    )\n",
    "    html = populate_template(html, \"[[ATTRIBUTIONS]]\", \"<p><i>Sources used: \" + \", \".join(attributions) + \"</i></p>\", condition=attributions)\n",
    "    html = populate_template(html, \"[[CREDITS_INTRO]]\", \"<h3>üíù Credits</h3>\" if (len(attributions) + len(car_talk_block)) > 0 else \"\") # No attribtions or Car Talk credits? No credits section at all!\n",
    "    \n",
    "    # Append logs to admin's email, if we're in prod mode\n",
    "    log_items = [l for l in log_stream.getvalue().split(\"\\n\") if len(l)>0] if log_stream else []\n",
    "    html = populate_template(html, \"[[LOGGING_BLOCK]]\", f\"<h3>üëæ Logs</h3>\", log_items, condition=issue_config[\"admin\"] is True and log_items)\n",
    "    \n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd77b956-f623-4659-bbe1-25a231789ee2",
   "metadata": {},
   "source": [
    "## üì∞ Publish\n",
    "Orchestrate and deliver the news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5edaf8-9f09-4528-bc6a-64de2778c024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def email_issue(sender, subscriber_email, html, images):\n",
    "    \"\"\"Send issue of Finite News to a subscriber by email using the SendGrid API service.\n",
    "    \n",
    "    NOTE\n",
    "    Requires a secret in AWS Secret Manager for SENDGRID_API_KEY\n",
    "    \n",
    "    ARGUMENTS\n",
    "    sender (dict): Metadata about the email source, with keys for \"subject\" and \"email\"\n",
    "    subscriber_email (str): The email address for the destination\n",
    "    html (str): The issue content\n",
    "    images (list): Optional, png images to attach to the email\n",
    "    \n",
    "    RETURNS\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    today = date.today().strftime(\"%m.%d.%y\").lstrip(\"0\")\n",
    "    message = Mail(\n",
    "        from_email=sender[\"email\"],\n",
    "        to_emails=subscriber_email,\n",
    "        subject=f\"{sender['subject']} for {today}\",\n",
    "        html_content=html\n",
    "    )\n",
    "    attachments = []\n",
    "    for i, image in enumerate(images):\n",
    "        attachedFile = Attachment(\n",
    "            disposition='inline',\n",
    "            file_name=f'image_{i}.png',\n",
    "            file_type='image/png',\n",
    "            file_content=image,\n",
    "            content_id=f'image_{i}',\n",
    "        )\n",
    "        attachments.append(attachedFile)\n",
    "    message.attachment = attachments\n",
    "    try:\n",
    "        sendgrid_key = get_fn_secret('SENDGRID_API_KEY')\n",
    "        sg = SendGridAPIClient(sendgrid_key)\n",
    "        response = sg.send(message)\n",
    "        if response.status_code==202:\n",
    "            logging.info(f\"{subscriber_email}: Extry extry! Email is away!\")\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"{subscriber_email}: Error in send_email: {str(type(e))}, {str(e)}\") # Admin issue will get this logging line in its email about failures in prior, non-admin issues.\n",
    "\n",
    "\n",
    "def deliver_issue(issue_config, html, images):\n",
    "    \"\"\"Send the content of Finite News to one subscriber by the selected method\n",
    "\n",
    "    ARGUMENTS\n",
    "    issue_config (dict): The settings for the issue\n",
    "    html (str): The content of the email formatted for the email\n",
    "    images (list): Optional, images to attach to the image\n",
    "    \n",
    "    RETURNS\n",
    "    None\n",
    "    \"\"\"\n",
    "    logging.info(f\"{issue_config['subscriber_email']}: Starting deliver_issue()\")\n",
    "    if issue_config[\"email_delivery\"]:\n",
    "        email_issue(\n",
    "            issue_config[\"sender\"],\n",
    "            issue_config[\"subscriber_email\"],\n",
    "            html,\n",
    "            images\n",
    "        )\n",
    "    else: \n",
    "        # Write issue to file\n",
    "        # Append issue's html to local .txt file that collect the day's issues. Creates file if it doesn't exist.\n",
    "        with open(f\"issues_for_{datetime.now().strftime('%m-%d-%y')}.txt\", \"a\") as f:\n",
    "            f.write(f\"\"\"{issue_config['subscriber_email']}\\n{datetime.now().strftime('%m-%d-%y %H:%M:%S')}\\n{html}\\n--------------------------------------------\\n\"\"\")\n",
    "        logging.info(f\"{issue_config['subscriber_email']}: Extry extry! Wrote to text file.\")\n",
    "\n",
    "\n",
    "def create_issue(issue_config, log_stream, dev_mode=False):   \n",
    "    \"\"\"Populate the content of Finite News customized for one subscriber\n",
    "    \n",
    "    ARGUMENTS\n",
    "    issue_config (dict): The settings for the issue\n",
    "    log_stream (StringIO object): In-memory file-like object that collects results from logging during the Finite News run\n",
    "    dev_mode (bool): If we're in dev/debug, output plots to local files too.\n",
    "\n",
    "    RETURNS\n",
    "    html (str): The content of the email formatted for the email\n",
    "    images (list): Optional, images to attach to the image\n",
    "    \"\"\"    \n",
    "    \n",
    "    logging.info(f\"{issue_config['subscriber_email']}: Starting create_issue()\")\n",
    "        \n",
    "    # Get news\n",
    "    news_headlines = [research_source(source, issue_config[\"requests_timeout\"]) for source in issue_config[\"news_sources\"]]\n",
    "    news_headlines = unnest_list(news_headlines)\n",
    "    news_headlines = dedup(news_headlines) # Dedup at source and aggregate level here too, because sometimes we get same headline from multiple sources, like if we pull from multiple sections of the same site\n",
    "    content_to_cache = news_headlines # Start collecting items we don't want to repeat in the next issue. Do before editing, which dedups using that cache.\n",
    "    headlines = edit_headlines(news_headlines, issue_config)\n",
    "\n",
    "    # Sports: Get tonight's games for tracked teams\n",
    "    # Note: These are not added to content_to_cache, so they are not cached in cache_path file or de-duped from the last issue\n",
    "    nba_headlines = [get_todays_nba_game(nba_team, issue_config[\"requests_timeout\"]) for nba_team in issue_config[\"sports\"].get(\"nba_teams\", [])]\n",
    "    nba_headlines = edit_sports_headlines(nba_headlines, issue_config[\"sports\"].get(\"nba_teams\", []))\n",
    "    nhl_headlines = [get_todays_nhl_game(team_place_name, issue_config[\"requests_timeout\"]) for team_place_name in issue_config[\"sports\"].get(\"nhl_teams\", [])]\n",
    "    nhl_headlines = edit_sports_headlines(nhl_headlines, issue_config[\"sports\"].get(\"nhl_teams\", []))\n",
    "    headlines = nba_headlines + nhl_headlines + headlines \n",
    "\n",
    "    # Get Alerts, a separate kind of headline that get edited more lightly\n",
    "    alerts = [research_source(source, issue_config[\"requests_timeout\"]) for source in issue_config[\"alerts_sources\"]]\n",
    "    alerts = unnest_list(alerts)\n",
    "    content_to_cache += alerts # Cache alerts so we don't repeat them in the next issue. Do before editing, which checks that cache.\n",
    "    alerts = edit_headlines(alerts, issue_config, filter_for_substance=False, smart_deduplicate=False) # Remove exact repeats, but don't try to remove non-substantive content or smart dedup (MBTA can have multiple, semantically similar alerts)\n",
    "\n",
    "    if issue_config[\"forecast\"]:\n",
    "        forecast = get_forecast(issue_config[\"forecast\"])\n",
    "    else:\n",
    "        forecast = None\n",
    "        \n",
    "    # Get Events section in HTML\n",
    "    events_html = \"\".join(\n",
    "            [research_source(source, issue_config[\"requests_timeout\"]) for source in issue_config[\"events_sources\"]]\n",
    "    )\n",
    "\n",
    "    # Get Stock plot images, if requested by subscriber \n",
    "    stock_plots = []\n",
    "    if len(issue_config[\"stocks\"])>0:\n",
    "        for tickers_set in issue_config[\"stocks\"]:\n",
    "            stock_plots.append(get_stocks_plot(tickers_set, issue_config[\"stocks_frequency\"], dev_mode))\n",
    "\n",
    "    # Other images\n",
    "    # A. images that we need to attach to the email\n",
    "    images = stock_plots + get_screenshots([source for source in issue_config[\"image_sources\"] if source[\"type\"]==\"screenshot\"])\n",
    "\n",
    "    # B. image_urls that we don't need to attach, the <img> html is sufficient\n",
    "    image_urls = [research_source(source, issue_config[\"requests_timeout\"]) for source in issue_config[\"image_sources\"] if source[\"type\"]==\"image_url\"]\n",
    "    image_urls = unnest_list([element for element in image_urls if element])\n",
    "    content_to_cache += image_urls\n",
    "    image_urls = edit_headlines(\n",
    "        image_urls,\n",
    "        issue_config,\n",
    "        filter_for_substance=False,\n",
    "        smart_deduplicate=False,\n",
    "        enforce_trailing_period=False\n",
    "    ) # Don't show the image if it was in the last issue\n",
    "\n",
    "    # Cache content, including unedited news headlines, sports headlines, alerts, and image_urls.\n",
    "    # Do so with originals before removing repeats, cleaning, or applying substance filters.\n",
    "        # That way, when checking cache for repeats, we compare unedited to unedited (same punctuation etc).\n",
    "        # Also GPT filtering is nondeterministic. We need to remove repeats before GPT changes the pool.\n",
    "    # But update the cache them _after_ edit_headlines(), which uses the cache to dedup and, at that point, needs to be the last issue's content, not this issue's.\n",
    "    if issue_config[\"editorial\"][\"cache_issue_content\"]: \n",
    "        cache_issue_content(\n",
    "            content_to_cache,\n",
    "            issue_config[\"bucket_path\"],\n",
    "            issue_config[\"editorial\"][\"cache_path\"]\n",
    "        )\n",
    "\n",
    "    # Pull it all together\n",
    "    html = format_issue(\n",
    "        content = {\n",
    "            \"headlines\": headlines,\n",
    "            \"alerts\": alerts,\n",
    "            \"forecast\": forecast,\n",
    "            \"events_html\": events_html,\n",
    "            \"stock_plots\": stock_plots,\n",
    "            \"images\": images,\n",
    "            \"image_urls\": image_urls\n",
    "        },\n",
    "        issue_config=issue_config,\n",
    "        log_stream=log_stream\n",
    "    )\n",
    "    logging.info(f\"{issue_config['subscriber_email']}: Finished create_issue()\")\n",
    "    return html, images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f8abf-e7f0-401e-8a8c-134fca5dae0c",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2120eca-1f10-4bc9-b96a-84f34662d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_finite_news(dev_mode, disable_gpt, logging_level):\n",
    "    \"\"\"Entry point to create and deliver issues to all subscribers of Finite News.\n",
    "\n",
    "    ARGUMENTS\n",
    "    dev_mode (bool): If True we're in development or debug mode, so don't send emails or modify headline_logs, and also output plots to local files.\n",
    "    disable_gpt (bool): If True, don't call the GPT API and incur costs, for example during dev or debug cycles.\n",
    "    logging_level (level from logging library): The deepest granularity of log messages to track\n",
    "    \n",
    "    RETURNS\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    log_stream = init_logging(logging_level, dev_mode)\n",
    "    for subscriber_config in tqdm(load_subscriber_configs(dev_mode, disable_gpt)):\n",
    "        try:\n",
    "            html, images = create_issue(subscriber_config, log_stream, dev_mode)\n",
    "            deliver_issue(subscriber_config, html, images)\n",
    "        except Exception as e:\n",
    "            if dev_mode: # During dev or debugging, raise exception and show traceback in notebook.\n",
    "                raise e\n",
    "            logging.critical(f\"{subscriber_config['subscriber_email']}: Issue failed due to unhandled exception. {traceback.format_exc()}\") # In prod mode, save traceback for admin's issue, but continue to try to publish the next issue.\n",
    "    print(\"üëç\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944739db-fe88-437e-8113-118ce4cb0ede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_finite_news(DEV_MODE, DISABLE_GPT, LOGGING_LEVEL)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
