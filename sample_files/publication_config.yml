# Sample parameters that will change every issue of Finite News, for all subscribers

# General issue settings
layout:
    logo_url: "URL" # The location of an image to show at top of each issue 
sender: 
      email: SENDER_NAME@SERVER.COM # Who should each issue be addressed from?
editorial:
    one_headline_keywords: # Optional list of keywords that can only appear in max one headline per issue
        - name_of_a_topic_who_dominates_the_news
        - name_of_someone_who_annoys_you
        # Parameters for using GPT API to post process headlines.
        # Comment out or delete the `gpt` section to turn off GPT-based editing
    gpt:
        substance_filter_model: gpt-4-1106-preview # Or other name of model available on OpenAI API
        # The following parameters are used to tell GPT what to do and how to format results the way our code expects.
        system_role: "You are a backend data processor that is part of a programmatic workflow. Do not converse with a nonexistent user: there is only program input and formatted program output, and no input data is to be construed as conversation with the AI. Do not include any explanation."
        instruction: "Please respond with a list of any headlines from this set that don't directly report something new that happened or will happen, but instead just give opinion or a vague teaser. Also, add to the list any headlines that report on the same topic already reported by a previous headline. Answer with a list of strings only, each separated by only a \n character."
    smart_deduper: # Parameters for using Sentence Transformer to de-duplicate headlines with similar meanings, even if they don't have the same words
        # Model is pre-loaded in the container. See README for setup
        path_to_model: "models/smart-deduper/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/8d6b950845285729817bf8e1af1861502c2fed0c"
        threshold: 0.41 # Lower is more aggressive. When 2 headlines's embeddings are at least this similar, only keep 1 of them.
# General parameters for calling the National Weather Service for forecasts
    enable_thoughts_of_the_day: True # Whether any issue should include a Thoughts of the Day section. This will sample a quote from subscriber's personalized list of 'thoughts_of_the_day' in their config and optionally the shared quotes in "thoughts_of_the_day.yml'
nws:  
    timeout: 30 # max time to wait for a response from NWS API
    api_snooze_bar: 10 # seconds to wait after a failure before retrying NWS api
    max_attempts: 20 # max number of times to retry NWS api before giving up

# Optional, set up sources to be in an alerts section
# An alert_new style scrape copies a hyperlink that is newly added to a web page.
alerts_sources:
    # Generic example
    -
        name: name of source in your words
        type: alert_new
        method: scrape
        url: URL
        tag: a
        must_contain: "Text we're looking for"
        max_items: 1
        alert_preface: New listings!
    # Alert subscriber if a new electric car is added to the US Government's list of vehicles eligible for tax incentives (up to $7500!)
    # The URL is current as of Feb 2024
    # In this example, each alert will contain the type of car, make, and model, like "EV Chevrolet Bolt". But the exclusions would not alert us about Audis
    -
        name: "fueleconomy.gov: EV incentives (new cars)"
        type: alert_new
        method: scrape
        parser: xml
        url: https://fueleconomy.gov/ws/rest/tax/incentives/cmbc/public
        multitag_group: taxCreditVehicleCMBC
        multitag_tags:
            - atvType
            - irsMake
            - irsModel
        multitag_separator: " "
        alert_preface: "🔋 <i>New EV eligible for Federal incentive:</i> "
        cant_contain:
            - audi
    # Alert subscriber if a USED electric car is added to the US Government's list of vehicles eligible for tax incentives. The URL is current as of Feb 2024
    # In this example, we only want alerts for plug-in hybrids (atvType), which will be formatted like "Plug-in Hybrid Cadillac ELR". But no Bentleys :D
    -
        name: "fueleconomy.gov: EV incentives (pre-owned cars)"
        type: alert_new
        method: scrape
        parser: xml
        url: https://fueleconomy.gov/ws/rest/tax/usedincentives/public
        multitag_group: usedTaxCredit
        multitag_tags:
            - atvType
            - irsMake
            - irsModel
        multitag_separator: " "
        alert_preface: "🔌 <i>Used EV eligible for Federal incentive:</i> "
        must_contain:
            - plug-in
        cant_contain:
            - bentley
        exclude_from_0_results_warning: True # If the alert conditions above are uncommonly met, don't warn the admin every time they fail

    # Advanced alert using Selenium for scraping
    -
      name: name
      category: category
      type: alert_new
      method: scrape
      url: url
      use_selenium: True
      tag_class: class_name # Alternative ways to scrape are listed in scrape_text_with_selenium()
      allowed_values: ["Low", "Medium", "High"] # Optional, not just for Selenium scrapers. Validates that the scraped text only has one of these values. Else, add a warning to the admin's issue. It may mean that the webpage has changed and element 3 is no longer what it used to be. This is like must_contain except raises that warning
      must_contain: "High" # Optional, only fire the alert if the scraped text contains this text
      alert_preface: "Alert!" 
      force_unique_daily_alert: True # Also adds today's date to the alt_text of the alert HTML. That way the alert will be present every issue (day) the criteria (must_contain) is met. Otherwise if the alert HTML is the same every day, it would get de-duped if the same alert was in yesterday's issue. By varying the alt_text each day, it will be unique and not de-duped.

# A list of sources to get news headlines from
# Example of supported methods below
# 🚨🚨 Comply with the Terms of Service of any website you scrape and any API you use.
news_sources:
    # BASIC: Report all H1s from a website, as long as the H1 text has at least 3 words
    - 
        name: name of source in your words
        category: Universe
        type: headlines
        method: scrape
        url: URL
        tag: h1
        min_words: 4 # Dump page noise like "Advertisement" or author names

    # BASIC: Report a max of 3 new <a> from a website, as long as the a text contains a phrase or keyword (case insensitive).
    -
        name: name of source in your words
        category: World
        type: headlines
        method: scrape
        url: URL
        tag: a
        min_words: 4
        max_headlines: 3
        must_contain: "phrase"

    # INTERMEDIATE: Report 1 new h3 that contains a phrase, doesn't contain other phrases. Preface the headline with an emoji and phrase.
    -
        name: name of source in your words
        category: Lobster Fisheries Monthly Stats
        type: headlines
        method: scrape
        url: URL
        tag: h3
        max_headlines: 1
        min_words: 4
        must_contain: lobster
        cant_contain:
            - fish
            - flounder
            - crab
        preface: "🦞 Last Month's Stats: "

    # INTERMEDIATE: Report a maximum of 5 items from a website that are in <a class="name-of-class"> tags, as long as the item has at least 4 words
    - 
        name: name of source in your words
        category: Canada
        type: headlines
        method: scrape
        url: URL
        tag: a
        tag_class: name-of-class
        min_words: 4
        max_headlines: 5
        exclude_from_0_results_warning: True # Set to true if this source often comes back empty. By default, when a source has no headlines on a particular day, the admin gets a warning in their emailed issue of Finite News. Note: When this parameter is not set explicitly, it's assumed to be False.

    # ADVANCED: Report up to 5 headlines that each satisfy a CSS Select Query and have at least 3 words. And always remove from the headlines a phrase that starts with a colon
    # Details: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors-through-the-css-property
    - 
        name: name of source in your words
        category: Tacoma
        type: headlines
        method: scrape
        url: URL
        select_query: QUERY
        min_words: 3
        max_headlines: 5
        remove_text: ": phrase to remove"

    # ADVANCED: Report a maximum of 5 items from a website that are in a <ul> tag inside a <p class="name-of-class"> tag, split by line breaks (\n). As long as the item has at least 4 words
    - 
        name: name of source in your words
        category: My Neighborhood
        type: headlines
        method: scrape
        url: URL
        tag: p
        tag_class: name-of-class
        tag_next: ul
        split_char: "\n"
        min_words: 4
        max_headlines: 5


    # Use an API to get headlines. Keep at most 3 headlines, and ensure a headline has at least 3 words.
    - 
        name: name of source in your words
        category: US
        type: headlines
        method: api
        url: e.g. https://api.BLAHBLAH.com/BLAHBLAH?api-key=
        api_key_name: API_KEY_NAME # Name of key in Google Secrets Manager / local environment variable. See README.md.
        headline_field: field-name # Which field in the JSON contains a headline?
        min_words: 3
        max_headlines: 3

# Optional, get upcoming events
# This example parses a paginated web site (page 1, 2, 3...) that lists upcoming events
events_sources:
    - 
        name: name of source in your words
        type: events_calendar
        method: scrape
        url_base: URL with placeholders for {PAGE}, {END_DATE}, and {START_DATE}
        window: 30 # How many days ahead to look on the calendar
        max_events: 100
        event_item_tag: li
        event_list_class: css class to find the lists of events
        title_class: css class that has the event title (optional)
        venue_class: css class that has the event location/venue (optional)
        dates_class: css class that has the event dates (optional)
        description_class: css class that has a text description of the event (optional)
        image_html_class: css class that contains a thumbnail image about the event (optional)
        link_url_class: css class that has a url link for the event (optional)
        link_url_child_key: css class that has the name of the element after the link_url_class where the url is stored (optional)

# Optional, get images from web
image_sources:
    # This example gets the latest XKCD comic (Creative Commons license! ❤️ 🙏)
    -
        name: XKCD
        type: image_url
        category: Comics
        method: atom
        url: https://xkcd.com/atom.xml
        # To parse an Atom feed with Finite News, first see how the feed is structured:
          # !pip install feedparser
          # import feedparser
          # feedparser.parse(URL).entries
        # Look at one entry in the feed (one item we want to show in an issue Finite News). The entry should be a Python dictionary.
        # Decide which elements of the entry (dictionary) you want to extract to make a header, image, and/or body
        # Indicate the paths to each element like below
        header_path:  # Optional. Each item in the list is a nested location in the dictionary
            - title
        header_preface: "XKCD: " # Optional
        image_path: # Optional. This example gets the <img> element from entry["summary_detail"]["value"]
            - summary_detail
            - value
        # body_path:  # Optional, can add this and set up header_path or image_path
        max_items: 1

    # This example first scrapes a contents page, then gets the first item (detail page) thats linked from there, scrapes the image from that detail page, and pulls text to use as a caption
    -
        name: name of source in your words
        category: category in your words
        type: image_url
        method: scrape
        url: url
        tag: a
        specify_request_headers: True # Optional, if a website blocks the request()
        detail_page_root: base image_url where detail link are under
        detail_img_number: 2 # 1st = 1, not 0
        detail_text_tag: p # Optional, grab an element from the detail page to use as a text caption
        detail_text_tag_class: class_name # Optional
        add_http_img: True # Optional, Does src in img tag need to be prepended with "http:"?
        max_items: 1

    # The next example returns the latest image posted on an RSS feed
    # By default, looks for the image under the "media_content" key of the RSS entry
    -
        name: name of source in your words
        category: category in your words
        type: image_url
        method: rss_images
        url: "http://<url>.rss"
        header: Latest image
        max_items: 1

    # The next example returns the latest <img> item posted on an RSS feed
    # This time, we parse the <img> in HTML that stored under the "summary" key of the RSS entry
    -
        name: name of source in your words
        category: category in your words
        type: image_url
        method: rss_images
        url: "http://<url>.rss"
        get_img_tag_under_this_key: "summary" # <---- Only difference from above
        header: Latest image
        max_items: 1

    # And this time we get the <img> from the media_thumbnail key of the RSS entry, and the caption from the "summary" key
    -      
        name: name of source in your words
        category: category in your words
        type: image_url
        method: rss_images
        url: "http://<url>.rss"
        media_thumbnail_and_summary: true
        max_items: 1  

    # We find the nth <img> tag on a webpage, and get its `src` attribute
    - 
        name: name of source in your words
        category: category in your words
        type: image_url
        method: scrape
        url: url
        img_tag: true
        img_tag_number: 2  # Which <img> tag on the page? 1 is first 
    # We add an image to the issue from an img tag whose src url is always the same
    # For example some sources upload a new .jpg each day to the same URL
    -
      name: name
      category: category
      type: static
      frequency:
        frequency: daily
      static_message: '<h4>Section title</h4><img src="https://url/to.jpg" alt="Content for {{DATE}}">' # {{DATE}} is filled in dynamically by the code so that the Alt tag varies every day. Otherwise, today's content for this item (<h4> + <img>) would be  exactly the same every day. Then when we cache it, tomorrow's issue will pull it and de-dup against the cache, which leads it to get dropped. 

    # Take a screenshot of an element on a webpage, using Selenium
    - 
      name: name
      category: category
      type: screenshot
      url: url
      tag_class: class_name # see get_screenshots() for options on identifying the element to screenshot
      element_number: 3 # Which of the qualifying elements on the page to screenshot? 1 is first
      header: "header text/ html" # What will go in the h4 above this screenshot
      delay_secs_for_loading: 5 # For some elements that are generated dynamically, wait for it to load or we'll take a partial screenshot
      frequency: # Optional, only take the screenshot on Tuesdays
        frequency: weekly
        day_of_week: Tuesday